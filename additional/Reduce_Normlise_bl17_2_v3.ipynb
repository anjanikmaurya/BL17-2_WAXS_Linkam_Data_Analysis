{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use env bl17_2 to run this script\n",
    "\n",
    "import param\n",
    "import panel as pn\n",
    "\n",
    "import holoviews as hv\n",
    "import imageio\n",
    "import os, glob, time\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_style(axs, fonts, xlabel, ylabel):\n",
    "    axs.set_xlabel(xlabel, fontsize=fonts)\n",
    "    axs.set_ylabel(ylabel, fontsize=fonts)\n",
    "    axs.tick_params(axis='both', which='major', direction='out', length=4, width=1)\n",
    "    axs.tick_params(which='minor', width=1, size=2)  # Adjust size as needed\n",
    "    axs.minorticks_on() # Turn on minor ticks\n",
    "    \n",
    "    #axs.grid(False, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
    "    axs.set_facecolor('white')\n",
    "    axs.spines['top'].set_linewidth(1)\n",
    "    axs.spines['right'].set_linewidth(1)\n",
    "    axs.spines['bottom'].set_linewidth(1)\n",
    "    axs.spines['left'].set_linewidth(1)\n",
    "    axs.tick_params(axis='x', labelsize=fonts)\n",
    "    axs.tick_params(axis='y', labelsize=fonts)\n",
    "\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import font\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyFAI\n",
    "\n",
    "import numpy as np\n",
    "import pyFAI\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def plot_2d_images(image, poni_file, output_folder, tif_file, azm_range, vmin=None, vmax=None):\n",
    "\n",
    "    # Load poni file to extract calibration parameters\n",
    "    ai = pyFAI.load(poni_file)\n",
    "    pixel_size = ai.pixel1  # Assuming square pixels\n",
    "    detector_distance = ai.dist\n",
    "\n",
    "    #print(ai)\n",
    "\n",
    "    #beamx = 1028.789\n",
    "    #beamy = 1126.865\n",
    "\n",
    "    beamx = ai.getFit2D()['centerX']\n",
    "    beamy = ai.getFit2D()['centerY']\n",
    "\n",
    "\n",
    "    #print(f\"beamx: {beamx}, beamy: {beamy}\")\n",
    "\n",
    "    wavelength = ai.wavelength\n",
    "\n",
    "    # Convert SAXS to q-space coordinates\n",
    "    x_pixels = image.shape[1]\n",
    "    y_pixels = image.shape[0]\n",
    "    x_coords = np.arange(x_pixels) - beamx\n",
    "    y_coords = np.arange(y_pixels) - beamy\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    qx = 1e-9 * 2 * np.pi / wavelength * np.sin(pixel_size * xx / detector_distance)\n",
    "    qy = 1e-9 * 2 * np.pi / wavelength * np.sin(pixel_size * yy / detector_distance)\n",
    "\n",
    "    # Calculate azimuthal angle (in degrees) for each pixel relative to the beam center\n",
    "    azimuthal_angles = np.degrees(np.arctan2(qy, qx))\n",
    "    \n",
    "    # Create a mask for the azimuthal angle range\n",
    "    mask = (azimuthal_angles >= azm_range[0]) & (azimuthal_angles <= azm_range[1])\n",
    "\n",
    "    # Plot the 2D image with the sector highlighted\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    # set nans to 0\n",
    "    image = np.nan_to_num(image, nan=0.0)\n",
    "    # Normalize the image  with sum of intensity\n",
    "    image = 10000000* (image / np.sum(image))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    im_avg = ax.pcolormesh(qx, qy, (image), cmap='jet', vmin=vmin, vmax=vmax, shading='auto')\n",
    "    \n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Overlay the sector by masking the area outside azimuthal range\n",
    "    #ax.contourf(qx, qy, mask, levels=[0.5, 1], colors='yellow', alpha=0.5)  # hatches=['//']\n",
    "\n",
    "    cbar_avg = fig.colorbar(im_avg, ax=ax, shrink=.8, label='Intensity [a.u.]')\n",
    "    cbar_avg.ax.tick_params(labelsize=20)\n",
    "    cbar_avg.ax.yaxis.label.set_size(20)\n",
    "    \n",
    "    ax.set_xlabel(r\"$q_x$ [nm$^{-1}$]\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$q_y$ [nm$^{-1}$]\", fontsize=20)\n",
    "    ax.set_title(f\"{f'{os.path.splitext(tif_file)[0]}_2D'}\", fontsize=14, y=1.05)\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Save and show the figure\n",
    "    # creat a Figures folder with output_folder\n",
    "    output_folder = os.path.join(output_folder, \"Figures\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # create the folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    # save the figure\n",
    "    plt.savefig(f\"{output_folder}/{f'{os.path.splitext(tif_file)[0]}'}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "def sort_files(filenames):\n",
    "    def extract_numbers(filename):\n",
    "        match = re.search(r'_s(\\d+)_0*(\\d+)', filename)\n",
    "        if match:\n",
    "            snumber = int(match.group(1))\n",
    "            number = int(match.group(2))\n",
    "            return snumber, number\n",
    "        return float('inf'), float('inf')  # Return large values if the pattern doesn't match\n",
    "\n",
    "    return sorted(filenames, key=extract_numbers)\n",
    "\n",
    "def extract_metadata(metadata):\n",
    "    # Extract the SPEC data\n",
    "    S = metadata['SPEC']['S']\n",
    "    i0 = S['1'] if '1' in S else None\n",
    "    i1 = S['2'] if '2' in S else None\n",
    "    Photod = S['4'] if '4' in S else None\n",
    "\n",
    "    # Extract the series_date\n",
    "    real_date_time = metadata['Eiger_metadata']['series_date']\n",
    "\n",
    "    return real_date_time, i0, i1, Photod\n",
    "\n",
    "#i0_1, i1_1, Photod_1 = extract_values(metadata_1)\n",
    "#i0_2, i1_2, Photod_2 = extract_values(metadata_2)\n",
    "\n",
    "def extract_metadata(metadata):\n",
    "    # Extract the SPEC data\n",
    "    S = metadata['SPEC']['S']\n",
    "    i0 = S['1'] if '1' in S else None\n",
    "    i1 = S['2'] if '2' in S else None\n",
    "    Photod = S['4'] if '4' in S else None\n",
    "\n",
    "    # Extract the series_date\n",
    "    real_date_time = metadata['Eiger_metadata']['series_date']\n",
    "\n",
    "    return real_date_time, i0, i1, Photod\n",
    "\n",
    "def extract_metadata(metadata):\n",
    "    timestamp = metadata['timestamp']\n",
    "    i0 = metadata['i0']\n",
    "    i1 = metadata['i1']\n",
    "    Photod = metadata['Photod']\n",
    "\n",
    "    return timestamp, i0, i1, Photod\n",
    "#i0_1, i1_1, Photod_1 = extract_values(metadata_1)\n",
    "#i0_2, i1_2, Photod_2 = extract_values(metadata_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from dateutil import parser\n",
    "\n",
    "import fabio\n",
    "from silx.io.specfile import SpecFile\n",
    "\n",
    "import pyFAI\n",
    "from pyFAI.detectors import Detector\n",
    "from pyFAI.azimuthalIntegrator import AzimuthalIntegrator\n",
    "\n",
    "import tifffile\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "tiff_fname = r'/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/thresh1_Erick_test_cscan1_Scan00003_s00000_00000.tif'\n",
    "\n",
    "img_meta = {}\n",
    "with tifffile.TiffFile(tiff_fname) as tif:\n",
    "    for tag in tif.pages[0].tags.values():\n",
    "        name, value = tag.name, tag.value\n",
    "        if name in img_meta.keys():\n",
    "            name += '_'\n",
    "        try:\n",
    "            img_meta[name] = json.loads(value)\n",
    "        except:\n",
    "            pass    \n",
    "\n",
    "default_motor_names = img_meta['ImageDescription']['SPEC']['Motor_names']\n",
    "default_counter_names = img_meta['ImageDescription']['SPEC']['Counter_names']\n",
    "\n",
    "\n",
    "def get_tiff_img_data(fname):\n",
    "    with tifffile.TiffFile(fname) as tif:\n",
    "        for idx, page in enumerate(tif.pages):\n",
    "            if idx == 0:\n",
    "                img_data = page.asarray().astype(float)\n",
    "            else:\n",
    "                img_data = np.concatenate((img_data, page.asarray().astype(float)))\n",
    "\n",
    "    img_data[img_data > 1e8] = np.nan\n",
    "    return img_data\n",
    "                           \n",
    "\n",
    "def get_tiff_meta_data(fname, df=None):                           \n",
    "    meta_dict = dict()\n",
    "    with tifffile.TiffFile(fname) as tif:\n",
    "        tif_tags = {}\n",
    "        for tag in tif.pages[0].tags.values():\n",
    "            name, value = tag.name, tag.value\n",
    "            if name in meta_dict.keys():\n",
    "                name += '_'\n",
    "            try:\n",
    "                meta_dict[name] = json.loads(value)\n",
    "            except:\n",
    "                pass     \n",
    "\n",
    "    # Get motor scanned\n",
    "    scan_motor = meta_dict['ImageDescription']['SPEC']['SCAN_COLS']['0']\n",
    "\n",
    "    # Get time when image acquired\n",
    "    series_start_time = meta_dict['ImageDescription']['Eiger_metadata']['series_date']\n",
    "    series_start_time = parser.parse(series_start_time).timestamp()\n",
    "    img_start_time = series_start_time + meta_dict['ImageDescription']['Eiger_metadata']['start_time']\n",
    "    \n",
    "    full_dict = {'timestamp' : img_start_time, 'scan_motor': scan_motor}\n",
    "        \n",
    "    try:\n",
    "        motor_names = meta_dict['ImageDescription']['SPEC']['Motor_names']\n",
    "    except:\n",
    "        motor_names = default_motor_names\n",
    "   \n",
    "    motor_vals = meta_dict['ImageDescription']['SPEC']['A']\n",
    "\n",
    "    try:\n",
    "        counter_names = meta_dict['ImageDescription']['SPEC']['Counter_names']\n",
    "    except:\n",
    "        counter_names = default_counter_names\n",
    "    counter_vals = meta_dict['ImageDescription']['SPEC']['S']\n",
    "    \n",
    "    motors = {mname:mval for ((km, mname), (kv, mval)) in zip(motor_names.items(), motor_vals.items())}\n",
    "    counters = {cname:cval for ((kc, cname), (kc, cval)) in zip(counter_names.items(), counter_vals.items())}\n",
    "    \n",
    "    full_dict.update({**motors, **counters})\n",
    "\n",
    "    try:\n",
    "        df = pd.concat([df, pd.DataFrame([full_dict])], ignore_index=True)\n",
    "    except:\n",
    "        df = pd.DataFrame([full_dict])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# azm and rad integration function: imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from isort import file\n",
    "import pyFAI\n",
    "import numpy as np  # Ensure this is included\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import fabio\n",
    "import h5py\n",
    "import json\n",
    "from pytools import F\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def integrate_tif_files(base_path, poni_file, mask_file, samp_folder, keyword, output_base_path, nsave2d=3, azm_range = (-60, 60), q1=None, q2=None, vmin=None, vmax=None,plot=False):\n",
    "    # Locate all 'Threshold' folders\n",
    "    #print(os.path.join(base_path, samp_folder, 'Threshold*'))\n",
    "    threshold_folders = glob.glob(os.path.join(base_path, samp_folder, keyword, 'Threshold 1'))\n",
    "    print(threshold_folders)\n",
    "    \n",
    "\n",
    "    if not threshold_folders:\n",
    "        raise FileNotFoundError(\"No Threshold folders found.\")\n",
    "    \n",
    "    # Initialize the PyFAI azimuthal integrator\n",
    "    ai = pyFAI.load(poni_file)\n",
    "\n",
    "    # Read the mask file with fabio\n",
    "    mask = fabio.open(mask_file).data\n",
    "\n",
    "    # Loop through each threshold folder\n",
    "    for threshold_folder in threshold_folders:\n",
    "        intermediate_folder = os.path.basename(os.path.dirname(threshold_folder))\n",
    "        output_folder = os.path.join(output_base_path, 'OneD_integrated_WAXS_01', samp_folder)#, intermediate_folder)\n",
    "        save_figures_folder = os.path.join(output_folder, 'Figures')\n",
    "        os.makedirs(save_figures_folder, exist_ok=True)\n",
    "        #output_folder = os.path.join(output_base_path, 'oneD_reducted', samp_folder, intermediate_folder)\n",
    "\n",
    "        #save_2d_folder = os.path.join(output_base_path, 'oneD_reducted','twoD')\n",
    "        #os.makedirs(save_2d_folder, exist_ok=True)\n",
    "\n",
    "        #save_1D_folder = os.path.join(output_base_path, 'oneD_reducted','oneD')\n",
    "\n",
    "        # Ensure the output folder exists\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        #txt_folder = os.path.join(output_base_path, 'D_txt')\n",
    "        #os.makedirs(txt_folder, exist_ok=True)\n",
    "\n",
    "        # List all .tif files in the current threshold folder\n",
    "        #tif_files = [f for f in os.listdir(threshold_folder) if f.endswith('.tif')]\n",
    "        tif_files = [f for f in os.listdir(threshold_folder) if f.endswith('.tif') and not f.startswith('._')]\n",
    "\n",
    "        \n",
    "        tif_files = sort_files(tif_files)\n",
    "\n",
    "        # sort files\n",
    "\n",
    "        #print(f\"tif_files: {tif_files}\")\n",
    "\n",
    "        # Initialize data containers for plotting\n",
    "        azimuthal_data = []\n",
    "        radial_data = []\n",
    "        \n",
    "        # Use tqdm for a progress bar with a shorter description\n",
    "        for i, tif_file in enumerate(tqdm(tif_files, desc='Processing', unit='file'), start=1):\n",
    "            \n",
    "            file_path = (os.path.join(threshold_folder, tif_file))\n",
    "            try:\n",
    "                img = imageio.v3.imread(file_path)\n",
    "                img[mask == 1] = 0\n",
    "                #print(img.shape, img.dtype)\n",
    "                # print (f\"img: {img}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue  # Skip the current file and move on to the next\n",
    "\n",
    "\n",
    "            #an_img = imageio.v3.immeta(file_path)\n",
    "            #print(file_path)\n",
    "            #eiger_metadata = json.loads(an_img['description'])\n",
    "            #eiger_metadata = json.loads(an_img['Eiger_metadata'])\n",
    "            #print(f\"eiger_metadata: {eiger_metadata}\")\n",
    "            \n",
    "            eiger_metadata_df = get_tiff_meta_data(file_path)\n",
    "\n",
    "            #print(f\"eiger_metadata: {eiger_metadata}\")\n",
    "            \n",
    "            real_date_time, i0, i1, Photod = extract_metadata(eiger_metadata_df)\n",
    "            normfactor = Photod * i1\n",
    "\n",
    "            #print(F\"real_date_time: {real_date_time}, i0: {i0}, i1: {i1}, Photod: {Photod}, normfactor: {normfactor}\")\n",
    "            #print(f'real_date_time: {real_date_time}, i0: {i0}, i1: {i1}, Photod: {Photod}, normfactor: {normfactor}')\n",
    "            #print(f\"i1: {i1}, Photod: {Photod}, normfactor: {normfactor}\")\n",
    "\n",
    "            #print(f\"real_date_time: {real_date_time}, i0: {i0}, i1: {i1}, Photod: {Photod}, normfactor: {normfactor}\")\n",
    "            \n",
    "            azmr = azm_range[1] - azm_range[0]\n",
    "            #print(f\"azm_range: {azm_range}, azmr: {azmr}\")\n",
    "            if i % nsave2d == 0:\n",
    "                plot_2d_images(img, poni_file, output_folder, tif_file, vmin=vmin, vmax=vmax, azm_range=azm_range)\n",
    "\n",
    "            # Azimuthal integration\n",
    "            #file_name = os.path.splitext(tif_file)[0] + '_azimuthal.txt'\n",
    "            result = ai.integrate1d(img, 4000, error_model='poisson', correctSolidAngle=True, azimuth_range = azm_range, normalization_factor=normfactor, polarization_factor=0.95, method='splitpixel', mask=mask)#,filename=os.path.join(txt_folder, file_name))\n",
    "            q, I_azimuthal, error_azm = result.radial, result.intensity, result.sigma\n",
    "            azimuthal_data.append((q, I_azimuthal, error_azm, tif_file, real_date_time, i0, i1, Photod))\n",
    "            \n",
    "            #plt.imshow(img, cmap='jet')\n",
    "            #print(f\"q: {q}, I_azimuthal: {I_azimuthal}, error_azm: {error_azm}\")\n",
    "            #file_name = os.path.splitext(tif_file)[0] + '_radial'\n",
    "            # Radial integration\n",
    "            chi, I_radial = ai.integrate_radial(\n",
    "                img,\n",
    "                npt=720, # number of points in the output pattern\n",
    "                #npt_rad=100,  # number of points in the radial space. Too few points may lead to huge rounding errors.\n",
    "                radial_range=(q1, q2), \n",
    "                azimuth_range=(-180, 0),\n",
    "                mask=mask,\n",
    "                normalization_factor=normfactor,\n",
    "                correctSolidAngle=True,\n",
    "                polarization_factor=0.95,\n",
    "                method='splitpixel',\n",
    "                unit='chi_deg',\n",
    "                radial_unit='q_nm^-1'\n",
    "                #filename=os.path.join(output_folder, file_name)\n",
    "            )\n",
    "            radial_data.append((chi, I_radial, tif_file, real_date_time, i0, i1, Photod))\n",
    "\n",
    "            # Combine chi and I_radial into a single array\n",
    "            #data_to_save = np.column_stack((chi, I_radial))\n",
    "\n",
    "            # Save the combined data to a single file\n",
    "            #np.savetxt(os.path.join(txt_folder, file_name + '.txt'), data_to_save, header=\"Chi, I_radial\")\n",
    "\n",
    "            #print(f\"chi: {chi}, I_radial: {I_radial}\")\n",
    "        # Save the azimuthal data in HDF5 format\n",
    "        \n",
    "        #output_folder_basename = os.path.basename(output_folder)\n",
    "        with h5py.File(os.path.join(output_folder, f'{intermediate_folder}_{azmr}_azimuthal_data.h5'), 'w') as hf:\n",
    "            for idx, (q, I, err, tif_file, real_date_time, i0, i1, Photod) in enumerate(azimuthal_data):\n",
    "                group = hf.create_group(f'data_{idx}')\n",
    "                group.create_dataset('q', data=q)\n",
    "                group.create_dataset('I', data=I)\n",
    "                group.create_dataset('error', data=err)\n",
    "                group.create_dataset('filename', data=(tif_file))\n",
    "                group.create_dataset('real_date_time', data=(real_date_time))\n",
    "                group.create_dataset('i0', data=np.float32(i0) if i0 is not None else np.nan)\n",
    "                group.create_dataset('i1', data=np.float32(i1) if i1 is not None else np.nan)\n",
    "                group.create_dataset('Photod', data=np.float32(Photod) if Photod is not None else np.nan)\n",
    "        \n",
    "        # Save the radial data in HDF5 format\n",
    "        #output_folder_basename = os.path.basename(output_folder)\n",
    "        with h5py.File(os.path.join(output_folder, f'{intermediate_folder}_radial_data.h5'), 'w') as hf:\n",
    "            for idx, (chi, I, tif_file, real_date_time, i0, i1, Photod) in enumerate(radial_data):\n",
    "                group = hf.create_group(f'data_{idx}')\n",
    "                group.create_dataset('chi', data=chi)\n",
    "                group.create_dataset('I', data=I)\n",
    "                group.create_dataset('filename', data=np.bytes_(tif_file))\n",
    "                group.create_dataset('real_date_time', data=np.bytes_(real_date_time))\n",
    "                group.create_dataset('i0', data=np.float32(i0) if i0 is not None else np.nan)\n",
    "                group.create_dataset('i1', data=np.float32(i1) if i1 is not None else np.nan)\n",
    "                group.create_dataset('Photod', data=np.float32(Photod) if Photod is not None else np.nan)\n",
    "        \n",
    "        # Plot the integrated data\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\n",
    "\n",
    "        #print(f\"azimuthal_data: {azimuthal_data}\")\n",
    "        \n",
    "        num_lines = len(azimuthal_data)\n",
    "        cmap = plt.cm.viridis  # You can change this to any colormap you like\n",
    "        norm = plt.Normalize(vmin=0, vmax=num_lines - 1)\n",
    "        ii =0 \n",
    "        # Plot azimuthal data\n",
    "        # condition to plot\n",
    "        for q, I, err, tif_file, *_ in azimuthal_data:\n",
    "            color = cmap(norm(ii))\n",
    "            ax1.plot(q, I, label=tif_file, color=color)\n",
    "            if q1 is not None:\n",
    "                ax1.axvline(x=q1, color='r', linestyle='--')\n",
    "            if q2 is not None:\n",
    "                ax1.axvline(x=q2, color='r', linestyle='--')\n",
    "            ii=ii+1\n",
    "        \n",
    "        ax1.set_xlabel('q [1/nm]')\n",
    "        ax1.set_xlim(4, 30)\n",
    "        ax1.set_ylabel('Intensity [a.u.]')\n",
    "        ax1.set_title('Azimuthal Integration')\n",
    "\n",
    "        set_plot_style(ax1, 20, 'q [1/nm]', 'Intensity [a.u.]')\n",
    "        #ax1.legend()\n",
    "        ii=0\n",
    "        # Plot radial data\n",
    "        for chi, I, tif_file, *_ in radial_data:\n",
    "            color = cmap(norm(ii))\n",
    "            ax2.plot(chi, I, 'o', markersize=2, label=tif_file,color=color)\n",
    "            ii=ii+1\n",
    "        ax2.set_xlabel('chi [degrees]')\n",
    "        ax2.set_ylabel('Intensity [a.u.]')\n",
    "        ax2.set_xlim(-180, 0)\n",
    "        #ax2.set_title('Radial Integration')\n",
    "        set_plot_style(ax2, 20, 'chi [degrees]', 'Intensity [a.u.]')\n",
    "        #ax2.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        title_name = [f.replace('thresh1_', '').replace('_s00000_00000.tif', '') for f in tif_files]\n",
    "\n",
    "        #plt.savefig(os.path.join(save_figures_folder, f'{title_name[0]}_heatmap_plot.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.savefig(os.path.join(save_figures_folder, f'{title_name[0]}_integrated_plot.png'), dpi=300, bbox_inches='tight')\n",
    "        #plt.close(fig)\n",
    "        if plot:\n",
    "            plt.show()\n",
    "\n",
    "            # Close to free memory\n",
    "            plt.close(fig)\n",
    "        #plt.show()\n",
    "\n",
    "        # write python code below to 3d plot of azimuthal data: take q as x, y as filenumber, z as I\n",
    "        # Example data structure for azimuthal_data\n",
    "        # Replace this with your actual azimuthal data\n",
    "        # azimuthal_data = [(q_array, I_array, err_array, tif_file), ...]\n",
    "\n",
    "        # Extract data for plotting\n",
    "        q_values = []\n",
    "        intensities = []\n",
    "        file_numbers = []\n",
    "\n",
    "        for file_index, (q, I, *_rest) in enumerate(azimuthal_data):\n",
    "            q_values.append(q)\n",
    "            intensities.append(I)\n",
    "            file_numbers.append(file_index)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        q_grid = np.array(q_values)\n",
    "        intensity_grid = np.array(intensities)\n",
    "        file_numbers = np.array(file_numbers)\n",
    "\n",
    "        # Generate a meshgrid for q and file numbers\n",
    "        X, Y = np.meshgrid(q_grid[0], file_numbers)  # Assuming all q arrays are the same\n",
    "        Z = intensity_grid\n",
    "\n",
    "        # Create the 3D plot\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Plot the surface\n",
    "        surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
    "\n",
    "        # Add colorbar\n",
    "        #colorbar = fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10)\n",
    "        #colorbar.set_label('Intensity')\n",
    "\n",
    "        # Set axis labels\n",
    "        ax.set_xlabel('q [nm⁻¹]')\n",
    "        ax.set_ylabel('File Number')\n",
    "        ax.set_zlabel('Intensity [a.u.]')\n",
    "        #ax.set_xlim(5, 30)\n",
    "        #set_plot_style(ax, 20, 'q [nm⁻¹]', 'Intensity [a.u.]')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        if plot:\n",
    "            plt.show()\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        # Assuming the azimuthal_data contains (q, I, *_rest)\n",
    "        # azimuthal_data = [(q_array, I_array, err_array, tif_file), ...]\n",
    "\n",
    "        # Extract data for plotting\n",
    "        q_values = []\n",
    "        intensities = []\n",
    "        file_numbers = []\n",
    "\n",
    "        for file_index, (q, I, *_rest) in enumerate(azimuthal_data):\n",
    "            q_values.append(q)\n",
    "            intensities.append(I)\n",
    "            file_numbers.append(file_index)\n",
    "\n",
    "        thickness = np.array(file_numbers)*10\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        q_grid = np.array(q_values)\n",
    "        intensity_grid = np.array(intensities)\n",
    "        thickness = np.array(thickness)\n",
    "\n",
    "        # Generate a meshgrid for q (X-axis) and file numbers (Y-axis)\n",
    "        X, Y = np.meshgrid(q_grid[0], thickness)  # Assuming all q arrays are the same length\n",
    "        Z = intensity_grid  # This will hold the intensities for the heatmap\n",
    "\n",
    "        # Create the figure\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "        # Create the pcolormesh plot\n",
    "        c = ax.pcolormesh(X, Y, Z, cmap='coolwarm', shading='auto')\n",
    "\n",
    "        # Add colorbar\n",
    "        # Create the colorbar without the fontsize argument\n",
    "        colorbar = fig.colorbar(c, ax=ax, label='Intensity [a.u.]', orientation='vertical', pad=0.02)\n",
    "\n",
    "        # Adjust the font size of the colorbar label\n",
    "        colorbar.set_label('Intensity [a.u.]', fontsize=20)\n",
    "\n",
    "        # Adjust the font size of the colorbar ticks\n",
    "        colorbar.ax.tick_params(labelsize=20)  # Adjust tick label size\n",
    "\n",
    "\n",
    "\n",
    "        # Set axis labels\n",
    "        ax.set_xlabel('q [nm⁻¹]')\n",
    "        ax.set_ylabel('Y [μm]')\n",
    "        ax.set_xlim(5, 30)\n",
    "        \n",
    "        # remove 'thresh1_f_P3HB_02_50RPM_as_spun_Scan00001_s00020_00000.tif'\n",
    "        # remore thresh1_ and _Scan00001_s00020_00000.tif from the filename\n",
    "\n",
    "        title_name = [f.replace('thresh1_', '').replace('_s00000_00000.tif', '') for f in tif_files]\n",
    "\n",
    "   \n",
    "        ax.set_title(title_name[0], fontsize=16, y=1.05)\n",
    "        set_plot_style(ax, 20, 'q [nm⁻¹]', 'Y [μm]')\n",
    "\n",
    "        #set_plot_style(ax, 20, 'q [nm⁻¹]', '#index')\n",
    "\n",
    "        # save the plot\n",
    "        plt.savefig(os.path.join(save_figures_folder, f'{title_name[0]}_heatmap_plot.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if plot:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read HDF5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_data(base_path, samp_folder, keyword, azimuthal=True):\n",
    "    \"\"\"\n",
    "    Read azimuthal or radial data from HDF5 files generated by integrate_tif_files.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str\n",
    "        Root path to the data.\n",
    "    samp_folder : str\n",
    "        Folder name corresponding to the sample.\n",
    "    keyword : str\n",
    "        Keyword like \"PS_500C\" etc.\n",
    "    azimuthal : bool, optional\n",
    "        If True, reads azimuthal data; otherwise reads radial data.\n",
    "\n",
    "    Returns:    \n",
    "    --------\n",
    "    data_list : list of dict\n",
    "        Each entry corresponds to one dataset stored in the HDF5 file.\n",
    "    \"\"\"\n",
    "    import h5py\n",
    "    import os\n",
    "    import glob\n",
    "\n",
    "    # Construct the expected folder path\n",
    "    target_folder = os.path.join(base_path, 'OneD_integrated_WAXS_01', samp_folder)\n",
    "    if not os.path.exists(target_folder):\n",
    "        raise FileNotFoundError(f\"Target folder does not exist: {target_folder}\")\n",
    "\n",
    "    # Locate HDF5 files\n",
    "    pattern = f\"*{keyword}*_azimuthal_data.h5\" if azimuthal else f\"*{keyword}*_radial_data.h5\"\n",
    "    h5_files = glob.glob(os.path.join(target_folder, pattern))\n",
    "\n",
    "    if not h5_files:\n",
    "        raise FileNotFoundError(f\"No matching HDF5 files found in {target_folder} with pattern {pattern}\")\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for h5_path in h5_files:\n",
    "        with h5py.File(h5_path, 'r') as hf:\n",
    "            for key in hf:\n",
    "                group = hf[key]\n",
    "                data = {\n",
    "                    \"filename\": group[\"filename\"][()].decode() if isinstance(group[\"filename\"][()], bytes) else group[\"filename\"][()],\n",
    "                    \"real_date_time\": group[\"real_date_time\"][()].decode() if isinstance(group[\"real_date_time\"][()], bytes) else group[\"real_date_time\"][()],\n",
    "                    \"i0\": group[\"i0\"][()],\n",
    "                    \"i1\": group[\"i1\"][()],\n",
    "                    \"Photod\": group[\"Photod\"][()]\n",
    "                }\n",
    "                if azimuthal:\n",
    "                    data.update({\n",
    "                        \"q\": group[\"q\"][()],\n",
    "                        \"I\": group[\"I\"][()],\n",
    "                        \"error\": group[\"error\"][()]\n",
    "                    })\n",
    "                else:\n",
    "                    data.update({\n",
    "                        \"chi\": group[\"chi\"][()],\n",
    "                        \"I\": group[\"I\"][()]\n",
    "                    })\n",
    "                data_list.append(data)\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ex-situ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base path and poni files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Base paths and file definitions\n",
    "#base_path = '/Users/akmaurya/Desktop/20240628/in_situ'  \n",
    "\n",
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/'\n",
    "poni_file = os.path.join(base_path, 'poni/poni_exsitu/LaB6_poni_New_01.poni')\n",
    "#mask_file = os.path.join(base_path, 'poni/poni_exsitu/exsitu_mask_new_09.edf')\n",
    "mask_file = os.path.join(base_path, 'poni/poni_insitu/mask_insitu_new_01.edf')\n",
    "output_base_path = base_path  # Base path for the output directory structure\n",
    "\n",
    "ai = pyFAI.load(poni_file) \n",
    "print (f\"ai: {ai}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHA3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.pyplot import plot\n",
    "import numpy as np\n",
    "# q-value range\n",
    "q1 = 9.25  # Minimum q-value (1/nm)\n",
    "q2 = 13 # Maximum q-value (1/nm)\n",
    "vmin, vmax = 0, 15  # P3HB\n",
    "\n",
    "\n",
    "\n",
    "folder = 'P3HB'\n",
    "\n",
    "folder_path = os.path.join(base_path, folder)\n",
    "\n",
    "# List of sample folders\n",
    "keywords = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "# filter the keywords with given pattern\n",
    "keywords = [f for f in keywords if 'Blank' in f]\n",
    "print(f\"folder_names: {keywords}\")\n",
    "#keywords=['cryomill_P3HB_powder_01_Scan00011','Blank_kaptontape_01_Scan00011','f_P3HB_02_50RPM_rep_as_spun_01_Scan00001','f_P3HB_03_100RPM_rep_as_spun_01_Scan00001','f_P3HB_04_200RPM_as_spun_01_Scan00005','f_P3HB_05_400RPM_as_spun_01_Scan00001','f_P3HB_06_600RPM_as_spun_01_Scan00001']\n",
    "\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "\n",
    "    samp_folder = f\"{keyword}\"\n",
    "    print(f\"Processing {samp_folder}...\")\n",
    "\n",
    "    integrate_tif_files(base_path, poni_file, mask_file,folder,keyword, output_base_path, nsave2d=100, azm_range = (-180, 0), q1=q1, q2=q2, vmin=vmin, vmax=vmax,plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "# q-value range   #### value of (100) peak at 18.15\n",
    "q1 = 17.9  # Minimum q-value (1/nm)\n",
    "q2 = 18.9  # Maximum q-value (1/nm)\n",
    "vmin, vmax = 0, 10  # P3HB\n",
    "#(17.9, 18.5)\n",
    "vmin, vmax = 0, 8  # PET\n",
    "\n",
    "\n",
    "\n",
    "folder = 'P3HB'\n",
    "\n",
    "folder_path = os.path.join(base_path, folder)\n",
    "\n",
    "keywords = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "# filter the keywords with given pattern\n",
    "keywords = [f for f in keywords if 'Blank' in f]\n",
    "print(f\"folder_names: {keywords}\")\n",
    "\n",
    "#keywords= ['cryomill_PET_powder_01_Scan00011','Run17_rep_f_PET_02_50rpm_as_spun_01_Scan00001','Run18_f_PET_03_100rpm_as_spun_01_Scan00001','Run19_f_PET_04_200rpm_as_spun_01_Scan00001','Run20_f_PET_05_400rpm_as_spun_01_Scan00001','Run21_f_PET_06_600rpm_as_spun_01_Scan00001']\n",
    "\n",
    "#keywords = []\n",
    "\n",
    "for keyword in keywords:\n",
    "\n",
    "    samp_folder = f\"{keyword}\"\n",
    "    print(f\"Processing {samp_folder}...\")\n",
    "\n",
    "    integrate_tif_files(base_path, poni_file, mask_file,folder,keyword, output_base_path, nsave2d=1000, azm_range = (-180, -0), q1=q1, q2=q2, vmin=vmin, vmax=vmax,plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anyio import key\n",
    "import numpy as np\n",
    "# q-value range\n",
    "q1 = 11  # Minimum q-value (1/nm)\n",
    "q2 = 12.5  # Maximum q-value (1/nm)\n",
    "\n",
    "\n",
    "vmin, vmax = 0,12  # Plla\n",
    "\n",
    "folder = 'P3HB'\n",
    "\n",
    "folder_path = os.path.join(base_path, folder)\n",
    "\n",
    "# List of sample folders\n",
    "keywords = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "#print(f\"folder_names: {keywords}\")\n",
    "keywords = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "# filter the keywords with given pattern\n",
    "keywords = [f for f in keywords if 'Blank' in f]\n",
    "print(f\"folder_names: {keywords}\")\n",
    "\n",
    "#keywords= ['cryomill_PLLA_powder_01_Scan00011','Run23_f_PLLA_02_50rpm_as_spun_01_Scan00001','Run24_f_PLLA_03_100rpm_as_spun_01_Scan00001','Run25_f_PLLA_04_200rpm_as_spun_01_Scan00001','Run26_f_PLLA_05_400rpm_as_spun_01_Scan00001','Run27_f_PLLA_06_600rpm_as_spun_01_Scan00001']\n",
    "#keywords = []\n",
    "#keywords=['cryomill_PLLA_powder_01_Scan00011']\n",
    "\n",
    "for keyword in keywords:\n",
    "\n",
    "    samp_folder = f\"{keyword}\"\n",
    "    print(f\"Processing {samp_folder}...\")\n",
    "\n",
    "    integrate_tif_files(base_path, poni_file, mask_file,folder,keyword, output_base_path, nsave2d=100, azm_range = (-105, -75), q1=q1, q2=q2, vmin=vmin, vmax=vmax, plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot azimuthal integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import fnmatch\n",
    "\n",
    "def plot_filtered_files(folder_path, patterns, xlim=(4, 20)):\n",
    "    \"\"\"\n",
    "    Plot data from text files in a given folder, filtered by wildcard patterns in filenames.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing the .txt files\n",
    "    - patterns (list): List of wildcard patterns to filter filenames\n",
    "    - xlim (tuple): x-axis limits for the plot\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6.5, 6), dpi=300)\n",
    "    plt.clf()\n",
    "    plt.ion()  # Enable interactive mode\n",
    "\n",
    "    \n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        if (\n",
    "            filename.endswith('.txt') and\n",
    "            'azimuthal' in filename and\n",
    "            not filename.startswith('._') and\n",
    "            any(fnmatch.fnmatch(filename, pattern) for pattern in patterns)\n",
    "        ):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "\n",
    "            try:\n",
    "                q_values = []\n",
    "                intensity_values = []\n",
    "\n",
    "                with open(file_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        # Skip unwanted lines\n",
    "                        if (\n",
    "                            line.startswith('#') or\n",
    "                            line.strip() == '' or\n",
    "                            'dtype' in line or\n",
    "                            'q:' in line or\n",
    "                            'intensity:' in line\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 2:\n",
    "                            q_values.append(float(parts[0]))\n",
    "                            intensity_values.append(float(parts[1]))\n",
    "\n",
    "                # Use part of the filename as label\n",
    "                label = filename.split('.')[0]  # Adjust if needed\n",
    "                print(f\"Filename: {label}\")\n",
    "                label = label.replace('thresh1_', '')\n",
    "                label = label.split('_Scan')[0]\n",
    "\n",
    "                if 'f_' in label:\n",
    "                    label = label.split('f_')[1]\n",
    "\n",
    "                if 'P3HB' in label:\n",
    "                    label = label.replace('P3HB', 'PHA-3B')\n",
    "\n",
    "                if 'rep_' in label:\n",
    "                    label = label.replace('rep_', '')\n",
    "                label = label.replace('_01', '')\n",
    "                #label = label.split('_f_')[1]\n",
    "                \n",
    "                # normlise intensity sum of total intensity\n",
    "                intensity_values = np.array(intensity_values)\n",
    "                intensity_values = 10000*intensity_values / np.sum(intensity_values)\n",
    "                #intensity_values = intensity_values / np.max(intensity_values)\n",
    "\n",
    "                \n",
    "                plt.plot(q_values, intensity_values, '-o',label=label, markersize=2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {file_path}: {e}\")\n",
    "        #print(f\"i: {i}\")\n",
    "    plt.xlabel('q [nm⁻¹]')\n",
    "    plt.ylabel('Intensity [a.u.]')\n",
    "\n",
    "\n",
    "    set_plot_style(plt.gca(), 20, 'q [nm⁻¹]', 'Intensity [a.u.]')\n",
    "   \n",
    "\n",
    "\n",
    "    plt.xlim(xlim)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "folder = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/D_txt/'\n",
    "#patterns = ['*PLLA_*s00010*', '*PLLA*_s000014*']\n",
    "\n",
    "patterns = ['*cryomill*PLLA*_s00005*',\n",
    "    '*PLLA*50rpm*s00010*',\n",
    "    '*PLLA*100rpm*s00010*',\n",
    "    '*PLLA*200rpm*s00010*',\n",
    "    '*PLLA*400rpm*s00010*',\n",
    "    '*PLLA*600rpm*s00010*',\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "plot_filtered_files(folder, patterns, xlim=(10, 15))\n",
    "\n",
    "patterns = ['*cryomill_P3HB_powder*_s00005*',\n",
    " '*f_P3HB_*_50RPM*_s00010*',\n",
    " '*f_P3HB_*_100RPM*_s00012*',\n",
    " '*f_P3HB_*_200RPM*_Scan00005_s00010*',\n",
    " '*f_P3HB_*_400RPM*_s00005*',\n",
    " '*f_P3HB_*_600RPM*_s00007*'\n",
    "]\n",
    "\n",
    "plot_filtered_files(folder, patterns, xlim=(3, 22)) \n",
    "\n",
    "patterns = ['*cryomill_PET_powder*_s00010*',\n",
    " '*PET_*_50rpm*_s00013*',\n",
    " '*PET_*_100rpm*_s00004*',\n",
    " '*PET_*_200rpm*_s00001*',\n",
    " '*PET_*_400rpm*_s00007*',\n",
    " '*PET_*_600rpm*_s00005*'\n",
    "]\n",
    "\n",
    "plot_filtered_files(folder, patterns, xlim=(3, 35)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot radial profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_plot_chi_intensity(folder_path, patterns, normalize=False):\n",
    "    \"\"\"\n",
    "    Searches for files matching patterns in the folder, reads Chi vs I_radial data, and plots.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing data files\n",
    "    - patterns (list): List of wildcard patterns to match filenames\n",
    "    - normalize (bool): If True, normalize the intensity values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6.5, 6), dpi=300)\n",
    "    plt.clf()\n",
    "    plt.ion()  # Enable interactive mode\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if (\n",
    "            filename.endswith('.txt')\n",
    "            and 'radial' in filename\n",
    "            and not filename.startswith('._')\n",
    "            and any(fnmatch.fnmatch(filename, pattern) for pattern in patterns)\n",
    "        ):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            chi = []\n",
    "            intensity = []\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line in file:\n",
    "                        # Skip header or empty lines\n",
    "                        if line.strip() == '' or line.startswith('#'):\n",
    "                            continue\n",
    "\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 2:\n",
    "                            chi.append(float(parts[0]))\n",
    "                            intensity.append(float(parts[1]))\n",
    "\n",
    "                if not chi:\n",
    "                    print(f\"No data found in {filename}. Skipping.\")\n",
    "                    continue\n",
    "                # mask intesity in given chi mask in -82 to -75 but pass out of of this range\n",
    "                chi = np.array(chi)\n",
    "                intensity = np.array(intensity)\n",
    "                mask = (chi <= -80) | (chi >= -74)\n",
    "                chi = chi[mask]\n",
    "                intensity = intensity[mask]\n",
    "                # Optional normalization\n",
    "                if normalize:\n",
    "                    max_intensity = max(intensity)\n",
    "                    #intensity = [i / max_intensity for i in intensity]\n",
    "\n",
    "                # Normalize intensity by sum of total intensity\n",
    "                intensity = np.array(intensity)\n",
    "                intensity = 10000 * intensity / np.sum(intensity)\n",
    "\n",
    "                # Use part of the filename as label\n",
    "                #label = filename.split('_')[2]  # Adjust index if needed\n",
    "                label = filename.split('.')[0]  # Adjust if needed\n",
    "                print(f\"Filename: {label}\")\n",
    "                label = label.replace('thresh1_', '')\n",
    "                label = label.split('_01_Scan')[0]\n",
    "                if 'f_' in label:\n",
    "                    label = label.split('f_')[1]\n",
    "                \n",
    "                if 'P3HB' in label:\n",
    "                    label = label.replace('P3HB', 'PHA-3B')\n",
    "\n",
    "                if 'rep_' in label:\n",
    "                    label = label.replace('rep_', '')\n",
    "                label = label.replace('_01', '')\n",
    "                \n",
    "                plt.plot(chi, intensity,'-o',label=label, markersize=2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "    plt.xlabel('Chi (degrees)')\n",
    "    plt.ylabel('Intensity (a.u.)')\n",
    "    plt.xlim(-120, -60)\n",
    "    #plt.ylim(2, 10)\n",
    "    set_plot_style(plt.gca(), 20, 'Chi [degrees]', 'Intensity [a.u.]')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "folder = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/D_txt/'\n",
    "patterns = [#'*cryomill*PLLA*_s00005*',\n",
    "    '*PLLA*50rpm*s00010*',\n",
    "    '*PLLA*100rpm*s00010*',\n",
    "    '*PLLA*200rpm*s00009*',\n",
    "    '*PLLA*400rpm*s00014*',\n",
    "    '*PLLA*600rpm*s00010*',\n",
    "    \n",
    "]\n",
    "read_and_plot_chi_intensity(folder, patterns, normalize=True)    \n",
    "\n",
    "patterns = [#'*cryomill_P3HB_powder*_s00010*',\n",
    " '*f_P3HB_*_50RPM*_s00010*',\n",
    " '*f_P3HB_*_100RPM*_s00012*',\n",
    " '*f_P3HB_*_200RPM*_Scan00005_s00010*',\n",
    " '*f_P3HB_*_400RPM*_s00005*',\n",
    " '*f_P3HB_*_600RPM*_s00007*'\n",
    "]\n",
    "\n",
    "read_and_plot_chi_intensity(folder, patterns, normalize=True) \n",
    "\n",
    "patterns = [#'*cryomill_PET_powder*_s00010*',\n",
    " '*PET_*_50rpm*_s00013*',\n",
    " '*PET_*_100rpm*_s00004*',\n",
    " '*PET_*_200rpm*_s00001*',\n",
    " '*PET_*_400rpm*_s00007*',\n",
    " '*PET_*_600rpm*_s00005*'\n",
    "]\n",
    "\n",
    "read_and_plot_chi_intensity(folder, patterns, normalize=True)     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot subtracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "def plot_subtracted_data(base_path, samp_folder, keyword, mode=\"azimuthal\", pattern_name=\"subtracted\"):\n",
    "    \"\"\"\n",
    "    Reads and plots data from HDF5 files whose names match the keyword pattern.\n",
    "    Also plots the dataset with the highest peak intensity from each file in a combined figure.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base directory path\n",
    "    - samp_folder (str): Sample folder inside D1\n",
    "    - keyword (str): Wildcard pattern to search for in filenames (e.g., '*P3HB*')\n",
    "    - mode (str): 'azimuthal' or 'chi'\n",
    "    - pattern_name (str): Prefix of the HDF5 file name (default: 'subtracted')\n",
    "    \"\"\"\n",
    "    subtracted_dir = os.path.join(base_path, 'D1', samp_folder, 'Subtracted_Data')\n",
    "    search_pattern = os.path.join(subtracted_dir, f\"{pattern_name}_{keyword}_{mode}.h5\")\n",
    "    matching_files = glob.glob(search_pattern)\n",
    "\n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"No file matching pattern: {search_pattern}\")\n",
    "\n",
    "    summary_data = []  # Store (x, I, filename) of max peak intensity for each file\n",
    "\n",
    "    for subtracted_file in matching_files:\n",
    "        print(f\"Plotting: {subtracted_file}\")\n",
    "        max_peak = -np.inf\n",
    "        best_x = best_I = best_filename = None\n",
    "\n",
    "        with h5py.File(subtracted_file, 'r') as hf:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            for key in hf.keys():\n",
    "                group = hf[key]\n",
    "                x = np.array(group[\"x\"])\n",
    "                I = np.array(group[\"I\"])\n",
    "                filename = group[\"filename\"][()].decode(\"utf-8\") if isinstance(group[\"filename\"][()], bytes) else group[\"filename\"][()]\n",
    "\n",
    "                plt.plot(x, I, 'o-', markersize=1, label='Subtracted')\n",
    "                plt.xlabel(\"q [1/nm]\" if mode == \"azimuthal\" else \"Chi [degrees]\")\n",
    "                plt.ylabel(\"Intensity [a.u.]\")\n",
    "                plt.title(f\"{filename}\")\n",
    "\n",
    "                # Check if this dataset has the highest peak\n",
    "                peak_intensity = np.max(I)\n",
    "                if peak_intensity > max_peak:\n",
    "                    max_peak = peak_intensity\n",
    "                    best_x = x\n",
    "                    best_I = I\n",
    "                    best_filename = filename\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if best_x is not None and best_I is not None:\n",
    "            summary_data.append((best_x, best_I, best_filename))\n",
    "\n",
    "    # Plot summary of highest peak datasets\n",
    "    if summary_data:\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        for x, I, filename in summary_data:\n",
    "            # Normalize intensity\n",
    "            I = 10000 * I / np.sum(I)\n",
    "            plt.plot(x+180, I, '-', label=os.path.basename(filename))\n",
    "        plt.xlabel(\"q [1/nm]\" if mode == \"azimuthal\" else \"Chi [degrees]\")\n",
    "        plt.ylabel(\"Intensity [a.u.]\")\n",
    "        plt.title(\"Highest Intensity Peak from Each File\")\n",
    "        #plt.xlim(5, 25)\n",
    "        set_plot_style(plt.gca(), 20, \"q [1/nm]\", \"Intensity [a.u.]\")\n",
    "        #plt.legend(fontsize='small')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/'\n",
    "samp_folder = 'P3HB'\n",
    "print(f\"Processing {samp_folder}...\")\n",
    "\n",
    "plot_subtracted_data(\n",
    "    base_path=base_path,\n",
    "    samp_folder=samp_folder,\n",
    "    keyword=\"*P3HB*\",\n",
    "    mode=\"azimuthal\"\n",
    ")\n",
    "\n",
    "\n",
    "plot_subtracted_data(\n",
    "    base_path=base_path,\n",
    "    samp_folder=samp_folder,\n",
    "    keyword=\"*PET*\",\n",
    "    mode=\"azimuthal\"\n",
    ")\n",
    "\n",
    "plot_subtracted_data(\n",
    "    base_path=base_path,\n",
    "    samp_folder=samp_folder,\n",
    "    keyword=\"*PLLA*\",\n",
    "    mode=\"azimuthal\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in-situ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Base paths and file definitions\n",
    "#base_path = '/Users/akmaurya/Desktop/20240628/in_situ'\n",
    "\n",
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/'\n",
    "poni_file = os.path.join(base_path, 'poni/poni_insitu/LaB6_insitu_01.poni')\n",
    "mask_file = os.path.join(base_path, 'poni/poni_insitu/mask_insitu_new_01.edf')\n",
    "output_base_path = base_path  # Base path for the output directory structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q-value range\n",
    "q1 = 14  # Minimum q-value (1/nm)\n",
    "q2 = 16  # Maximum q-value (1/nm)\n",
    "vmin, vmax = 0, 2  # Define vmin and vmax as needed\n",
    "folder = 'insitu'\n",
    "# List of sample folders\n",
    "keywords = [#'time_stamp_check_Scan00002', \n",
    "            #'insitu_Run1_LDPE_5mmmin_01_Scan00001', \n",
    "            'insitu_Run2_LDPE_5mmmin_01_Scan00001', \n",
    "            #'radiation_damage_abs101_elongatedLDPE_01_Scan00002', \n",
    "            #'radiation_damage_abs101_elongatedLDPE_01_Scan00003', \n",
    "            #'insitu_Run3_PHPD_film_base_5mmmin_01_Scan00001', \n",
    "            #'insitu_Run3_PHPD_film_base_5mmmin_01_Scan00002', \n",
    "            #'insitu_Run4_PHPD_film_base_5mmmin_01_Scan00001', \n",
    "            #'insitu_Run5_PHPD_film_base_5mmmin_01_Scan00001', \n",
    "            'insitu_Run6_PHPD_film_base_5mmmin_01_Scan00001', \n",
    "            #'insitu_Run7_PHPD_film_base_3mmmin_01_Scan00001', \n",
    "            'insitu_Run8_PHPD_film_base_3mmmin_01_Scan00001', \n",
    "            'insitu_Run9_PHPD_MBfilm_3mmmin_01_Scan00001', \n",
    "            'insitu_Run10_P5HV_film_3mmmin_01_Scan00001', \n",
    "            'insitu_Run11_air_with_TS600_01_Scan00001', \n",
    "            'insitu_Run11_nobeam_with_TS600_01_Scan00001', \n",
    "            #'insitu_Run12_LaB6_01_Scan00001', \n",
    "            #'insitu_Run12_LaB6_02_Scan00001', \n",
    "            'insitu_Run12_air_with_Scan00001', \n",
    "            'insitu_Run14_P5HV_film_3mmmin_01_Scan00001', \n",
    "            'insitu_Run15_P5HV_film_3mmmin_01_Scan00001', \n",
    "            'insitu_Run16_LDPE_film_3mmmin_01_Scan00001', \n",
    "            'insitu_Run17_P5HV_film_5mmmin_01_Scan00001']\n",
    "\n",
    "for keyword in keywords:\n",
    "    samp_folder = f\"{keyword}\"\n",
    "    print(f\"Processing {samp_folder}...\")\n",
    "    integrate_tif_files(base_path, poni_file, mask_file,folder,keyword, output_base_path, nsave2d=30, azm_range = (-45, 45), q1=q1, q2=q2, vmin=vmin, vmax=vmax)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bl17_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
