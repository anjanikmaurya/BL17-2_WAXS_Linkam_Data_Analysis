{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use env bl17_2 to run this script\n",
    "\n",
    "import param\n",
    "import panel as pn\n",
    "\n",
    "import holoviews as hv\n",
    "import imageio\n",
    "import os, glob, time\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "\n",
    "from tkinter import font\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyFAI\n",
    "\n",
    "import numpy as np\n",
    "import pyFAI\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_style(axs, fonts, xlabel, ylabel):\n",
    "    axs.set_xlabel(xlabel, fontsize=fonts)\n",
    "    axs.set_ylabel(ylabel, fontsize=fonts)\n",
    "    axs.tick_params(axis='both', which='major', direction='out', length=4, width=1)\n",
    "    axs.tick_params(which='minor', width=1, size=2)  # Adjust size as needed\n",
    "    axs.minorticks_on() # Turn on minor ticks\n",
    "    \n",
    "    #axs.grid(False, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
    "    axs.set_facecolor('white')\n",
    "    axs.spines['top'].set_linewidth(1)\n",
    "    axs.spines['right'].set_linewidth(1)\n",
    "    axs.spines['bottom'].set_linewidth(1)\n",
    "    axs.spines['left'].set_linewidth(1)\n",
    "    axs.tick_params(axis='x', labelsize=fonts)\n",
    "    axs.tick_params(axis='y', labelsize=fonts)\n",
    "\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save subtracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_subtracted_txt(q, I_sub, I_sub_sigma, I_raw, fname_sample, fname_bkg, fname_empty, timer, bstop, ctemp, I0, out_dir):\n",
    "    \"\"\"\n",
    "    Save subtracted WAXS data to a .dat text file with headers.\n",
    "\n",
    "    Parameters:\n",
    "    - q: array-like, q values in nm^-1\n",
    "    - I_sub: array-like, background-subtracted intensity\n",
    "    - I_sub_sigma: array-like, sigma of subtracted intensity\n",
    "    - I_raw: array-like, raw or normalized sample intensity\n",
    "    - fname_sample: str, sample filename\n",
    "    - fname_bkg: str, background filename\n",
    "    - fname_empty: str, empty scan filename\n",
    "    - timer: float or str, exposure time\n",
    "    - bstop: float or str, beamstop position\n",
    "    - ctemp: float or str, camera temperature\n",
    "    - I0: float or str, incident beam intensity\n",
    "    - out_dir: str, output directory\n",
    "    \"\"\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        \"q_nm^-1\": q,\n",
    "        \"I_avg_sub\": I_sub,\n",
    "        \"I_avg_sub_sigma\": I_sub_sigma,\n",
    "        \"I_sample\": I_raw,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Output file name\n",
    "    out_file = os.path.join(out_dir, f\"{fname_sample.split('.dat')[0]}_sub.dat\")\n",
    "\n",
    "    # Header lines\n",
    "    inst_info = f\"Timer: {timer}, bstop: {bstop}, ctemp: {ctemp}, I0: {I0}\"\n",
    "    headers = [\n",
    "        f\"filename: {fname_sample}\",\n",
    "        f\"background : {fname_bkg}\",\n",
    "        f\"Empty : {fname_empty}\",\n",
    "        inst_info,\n",
    "        \"fit_data\",\n",
    "        \"q_nm^-1 ------ I_avg_subtracted ------ I_avg_subtracted_sigma ------ I_Normalized\",\n",
    "    ]\n",
    "    commented = ['# ' + line for line in headers]\n",
    "\n",
    "    # Write file\n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write('\\n'.join(commented) + '\\n')\n",
    "        df.to_csv(f, sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_subtracted_txt(q, I_sub, I_sub_sigma, I_raw, fname_sample, fname_bkg, fname_empty, timer, bstop, ctemp, I0, out_dir):\n",
    "    \"\"\"\n",
    "    Save subtracted WAXS data to a .dat text file with headers.\n",
    "\n",
    "    Parameters:\n",
    "    - q: array-like, q values in nm^-1\n",
    "    - I_sub: array-like, background-subtracted intensity\n",
    "    - I_sub_sigma: array-like, sigma of subtracted intensity\n",
    "    - I_raw: array-like, raw or normalized sample intensity\n",
    "    - fname_sample: str, sample filename\n",
    "    - fname_bkg: str, background filename\n",
    "    - fname_empty: str, empty scan filename\n",
    "    - timer: float or str, exposure time\n",
    "    - bstop: float or str, beamstop position\n",
    "    - ctemp: float or str, camera temperature\n",
    "    - I0: float or str, incident beam intensity\n",
    "    - out_dir: str, output directory\n",
    "    \"\"\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        \"q_nm^-1\": q,\n",
    "        \"I_avg_sub\": I_sub,\n",
    "        \"I_avg_sub_sigma\": I_sub_sigma,\n",
    "        \"I_sample\": I_raw,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Output file name\n",
    "    out_file = os.path.join(out_dir, f\"{fname_sample.split('.dat')[0]}_sub.dat\")\n",
    "\n",
    "    # Header lines\n",
    "    inst_info = f\"Timer: {timer}, bstop: {bstop}, ctemp: {ctemp}, I0: {I0}\"\n",
    "    headers = [\n",
    "        f\"filename: {fname_sample}\",\n",
    "        f\"background : {fname_bkg}\",\n",
    "        f\"Empty : {fname_empty}\",\n",
    "        inst_info,\n",
    "        \"fit_data\",\n",
    "        \"q_nm^-1 ------ I_avg_subtracted ------ I_avg_subtracted_sigma ------ I_Normalized\",\n",
    "    ]\n",
    "    commented = ['# ' + line for line in headers]\n",
    "\n",
    "    # Write file\n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write('\\n'.join(commented) + '\\n')\n",
    "        df.to_csv(f, sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read HDF5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_data(base_path, samp_folder, keyword, azimuthal=True):\n",
    "    \"\"\"\n",
    "    Read azimuthal or radial data from HDF5 files generated by integrate_tif_files.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str\n",
    "        Root path to the data.\n",
    "    samp_folder : str\n",
    "        Folder name corresponding to the sample.\n",
    "    keyword : str\n",
    "        Keyword like \"PS_500C\" etc.\n",
    "    azimuthal : bool, optional\n",
    "        If True, reads azimuthal data; otherwise reads radial data.\n",
    "\n",
    "    Returns:    \n",
    "    --------\n",
    "    data_list : list of dict\n",
    "        Each entry corresponds to one dataset stored in the HDF5 file.\n",
    "    \"\"\"\n",
    "    import h5py\n",
    "    import os\n",
    "    import glob\n",
    "\n",
    "    # Construct the expected folder path\n",
    "    target_folder = os.path.join(base_path, samp_folder)\n",
    "    if not os.path.exists(target_folder):\n",
    "        raise FileNotFoundError(f\"Target folder does not exist: {target_folder}\")\n",
    "\n",
    "    # Locate HDF5 files\n",
    "    pattern = f\"*{keyword}*_azimuthal_data.h5\" if azimuthal else f\"*{keyword}*_radial_data.h5\"\n",
    "    h5_files = glob.glob(os.path.join(target_folder, pattern))\n",
    "\n",
    "    if not h5_files:\n",
    "        raise FileNotFoundError(f\"No matching HDF5 files found in {target_folder} with pattern {pattern}\")\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for h5_path in h5_files:\n",
    "        with h5py.File(h5_path, 'r') as hf:\n",
    "            for key in hf:\n",
    "                group = hf[key]\n",
    "\n",
    "                def get_safe_str(name):\n",
    "                    val = group.get(name)\n",
    "                    if val is not None:\n",
    "                        val = val[()]\n",
    "                        return val.decode() if isinstance(val, bytes) else val\n",
    "                    return \"N/A\"\n",
    "\n",
    "                def get_safe_val(name):\n",
    "                    val = group.get(name)\n",
    "                    return val[()] if val is not None else None\n",
    "\n",
    "                data = {\n",
    "                    \"filename\": get_safe_str(\"filename\"),\n",
    "                    \"real_date_time\": get_safe_str(\"real_date_time\"),\n",
    "                    \"i0\": get_safe_val(\"i0\"),\n",
    "                    \"i1\": get_safe_val(\"i1\"),\n",
    "                    \"Photod\": get_safe_val(\"Photod\")\n",
    "                }\n",
    "\n",
    "                if azimuthal:\n",
    "                    data.update({\n",
    "                        \"q\": group[\"q\"][()],\n",
    "                        \"I\": group[\"I\"][()],\n",
    "                        \"error\": group[\"error\"][()]\n",
    "                    })\n",
    "                else:\n",
    "                    data.update({\n",
    "                        \"chi\": group[\"chi\"][()],\n",
    "                        \"I\": group[\"I\"][()]\n",
    "                    })\n",
    "\n",
    "                data_list.append(data)\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def write_sorted_data_to_h5(sorted_data, output_file, mode='unknown'):\n",
    "    \"\"\"\n",
    "    Write sorted peak data into an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    sorted_data : list of tuples\n",
    "        Each tuple should contain (peak_intensity, x_array, I_array, filename, scan_number).\n",
    "        Missing values will be replaced with default 0 or empty array.\n",
    "    output_file : str\n",
    "        Full path where the HDF5 file will be saved.\n",
    "    mode : str, optional\n",
    "        Type or mode to store in the dataset ('radial', 'azimuthal', etc.). Default is 'unknown'.\n",
    "    \"\"\"\n",
    "    with h5py.File(output_file, 'w') as hf:\n",
    "        for idx, entry in enumerate(sorted_data):\n",
    "            # Fill missing values with defaults\n",
    "            peak = entry[0] if len(entry) > 0 else 0\n",
    "            x = np.array(entry[1]) if len(entry) > 1 else np.array([0])\n",
    "            I = np.array(entry[2]) if len(entry) > 2 else np.array([0])\n",
    "            filename = str(entry[3]) if len(entry) > 3 else 'unknown'\n",
    "            scan_number = entry[4] if len(entry) > 4 else 0\n",
    "\n",
    "            group = hf.create_group(f'data_{idx}')\n",
    "            group.create_dataset('x', data=x)\n",
    "            group.create_dataset('I', data=I)\n",
    "            group.create_dataset('filename', data=filename)\n",
    "            group.create_dataset('peak_intensity', data=peak)\n",
    "            group.create_dataset('type', data=str(mode))\n",
    "            group.create_dataset('scan_number', data=scan_number)\n",
    "            group.create_dataset('background_filename', data=filename)\n",
    "            group.create_dataset('background_file', data=filename)\n",
    "\n",
    "    print(f\"✅ Successfully wrote {len(sorted_data)} entries to '{output_file}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subtract function v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import scale\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "def subtract_background_v2(\n",
    "    base_path,\n",
    "    samp_folder,\n",
    "    bkg_folder,\n",
    "    keyword,\n",
    "    background_keyword,\n",
    "    subtract_azimuthal=True,\n",
    "    subtract_chi=False,\n",
    "    scaling_factor=1.0,\n",
    "    plot=True,\n",
    "    use_sample_as_background=False,\n",
    "    background_filename=None,\n",
    "    match_partial=False\n",
    "):\n",
    "    def process_subtraction(sample_data, background_data, x_key, save_suffix):\n",
    "        subtracted_data = []\n",
    "\n",
    "        for samp, bg in zip(sample_data, background_data):\n",
    "            print(f\"Subtracting background from sample:\")\n",
    "            print(f\"  Sample file     : {samp['filename']}\")\n",
    "            print(f\"  Background file : {bg['filename']}\\n\")\n",
    "\n",
    "            x = samp[x_key]\n",
    "            I_sample = np.array(samp[\"I\"])\n",
    "\n",
    "\n",
    "            if subtract_azimuthal and \"error\" in samp:\n",
    "                err = samp[\"error\"]\n",
    "            else:\n",
    "                err = None\n",
    "\n",
    "            # find the average of I values in the range of 8-10 q range\n",
    "            if x_key == \"q\":\n",
    "                q_range = (x >= 4) & (x <= 7)\n",
    "                if np.any(q_range):\n",
    "                    avg_I = np.mean(I_sample[q_range])\n",
    "                    avg_I_bg = np.mean(bg[\"I\"][q_range])\n",
    "                    print(f\"Average I in q range 8-10: {avg_I}\")\n",
    "                    print(f\"Average I in background q range 8-10: {avg_I_bg}\")\n",
    "                    if avg_I_bg > 0:\n",
    "                        scaling_factor = 0.98*(avg_I / avg_I_bg)\n",
    "                        print(f\"Scaling factor for azimuthal data: {scaling_factor}\")\n",
    "                    else:\n",
    "                        print(\"Warning: Average I in background is zero, using default scaling factor.\")\n",
    "                        scaling_factor = 1.0\n",
    "                else:\n",
    "                    print(\"Warning: No valid q range found for scaling.\")\n",
    "                    scaling_factor = 1.0\n",
    "            \n",
    "            else:\n",
    "                # For chi data, we can use a different range or method\n",
    "                chi_range = (x >= -3) & (x <= 0)\n",
    "                if np.any(chi_range):\n",
    "                    avg_I = np.mean(I_sample[chi_range])\n",
    "                    avg_I_bg = np.mean(bg[\"I\"][chi_range])\n",
    "                    print(f\"Average I in chi range 8-10: {avg_I}\")\n",
    "                    print(f\"Average I in background chi range 8-10: {avg_I_bg}\")\n",
    "                    if avg_I_bg > 0:\n",
    "                        scaling_factor = 0 #*(avg_I / avg_I_bg)\n",
    "                        print(f\"Scaling factor for chi data: {scaling_factor}\")\n",
    "                    else:\n",
    "                        print(\"Warning: Average I in background is zero, using default scaling factor.\")    \n",
    "                        # Use the average of the sample data for scaling\n",
    "                        #print(f\"Scaling factor for azimuthal data: {scaling_factor}\")\n",
    "                        scaling_factor = 0\n",
    "                else:\n",
    "                    print(\"Warning: No valid q range found for scaling.\")\n",
    "                    scaling_factor = 0\n",
    "            I_background = np.array(bg[\"I\"]) * scaling_factor\n",
    "\n",
    "            real_date_time = samp[\"real_date_time\"]\n",
    "            i0 = samp[\"i0\"]\n",
    "            i1 = samp[\"i1\"]\n",
    "            Photod = samp[\"Photod\"]\n",
    "\n",
    "            I_subtracted = I_sample - I_background\n",
    "\n",
    "\n",
    "            subtracted_data.append({\n",
    "                x_key: x,\n",
    "                \"I\": I_subtracted,\n",
    "                \"filename\": samp[\"filename\"],\n",
    "                \"err\": err,\n",
    "            })\n",
    "\n",
    "\n",
    "            if plot:\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                plt.plot(x, I_sample, '-', markersize=1, label='Sample')\n",
    "                plt.plot(x, I_background, '-', markersize=1, label='Background')\n",
    "                plt.plot(x, I_subtracted, '-', markersize=1, label='Subtracted')\n",
    "                plt.xlabel(x_key + ' [1/nm]' if x_key == \"q\" else 'Chi [degrees]')\n",
    "                plt.ylabel('Intensity [a.u.]')\n",
    "                plt.title(f\"Subtraction: {samp['filename']}\")\n",
    "                set_plot_style(plt.gca(), 20, x_key + ' [1/nm]' if x_key == \"q\" else 'Chi [degrees]', 'Intensity [a.u.]')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        # Save to HDF5\n",
    "        save_folder = os.path.join(base_path)\n",
    "        output_folder = os.path.join(save_folder, 'Subtracted_Data')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder, f'subtracted_{keyword}_{save_suffix}.h5')\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"File already exists: {output_file}. Overwriting.\")\n",
    "        else:\n",
    "            print(f\"Creating new file: {output_file}\")\n",
    "        \n",
    "        print(f\"Saving subtracted data to {output_file}\")\n",
    "\n",
    "        with h5py.File(output_file, 'w') as hf:\n",
    "            for idx, data in enumerate(subtracted_data):\n",
    "                group = hf.create_group(f'data_{idx}')\n",
    "                \n",
    "                # Determine x_key: either 'q' or 'chi'\n",
    "                x_key = 'q' if 'q' in data else 'chi'\n",
    "                \n",
    "                # Save both 'x' (generic) and specific axis ('q' or 'chi')\n",
    "                group.create_dataset('x', data=data[x_key])\n",
    "                group.create_dataset(x_key, data=data[x_key])\n",
    "                # Replace 'azimuthal' with this in HDF5 writing section:\n",
    "                if x_key == 'q' and 'err' in data:\n",
    "                    group.create_dataset('error', data=data['err'])\n",
    "                \n",
    "                group.create_dataset('I', data=data[\"I\"])\n",
    "                group.create_dataset('filename', data=data[\"filename\"])\n",
    "                group.create_dataset('scaling_factor', data=scaling_factor)\n",
    "                group.create_dataset('type', data=save_suffix)\n",
    "                group.create_dataset('background_filename', data=background_filename if use_sample_as_background else background_keyword)\n",
    "                group.create_dataset('background_file', data=background_data[idx]['filename'])\n",
    "                group.create_dataset('real_date_time', data=(real_date_time))\n",
    "                group.create_dataset('i0', data=np.float32(i0) if i0 is not None else np.nan)\n",
    "                group.create_dataset('i1', data=np.float32(i1) if i1 is not None else np.nan)\n",
    "                group.create_dataset('Photod', data=np.float32(Photod) if Photod is not None else np.nan)\n",
    "\n",
    "        return subtracted_data\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for mode, x_key, flag in [\n",
    "        (\"azimuthal_data\", \"q\", subtract_azimuthal),\n",
    "        (\"radial_data\", \"chi\", subtract_chi)\n",
    "    ]:\n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        # Read sample data\n",
    "        sample_data = read_h5_data(base_path, samp_folder, keyword, azimuthal=(x_key == \"q\"))\n",
    "        sample_data = sorted(sample_data, key=lambda x: x['filename'])\n",
    "\n",
    "        if use_sample_as_background:\n",
    "            if background_filename is None:\n",
    "                raise ValueError(\"Please specify 'background_filename' when using a sample file as background.\")\n",
    "\n",
    "            selected_bg = None\n",
    "            for data in sample_data:\n",
    "                fname = data[\"filename\"]\n",
    "                if (fname == background_filename) or (match_partial and background_filename in fname):\n",
    "                    selected_bg = data\n",
    "                    break\n",
    "\n",
    "            if selected_bg is None:\n",
    "                print(\"Available filenames:\")\n",
    "                for d in sample_data:\n",
    "                    print(\" -\", d[\"filename\"])\n",
    "                raise ValueError(f\"No sample file matches background_filename='{background_filename}'.\")\n",
    "\n",
    "            background_data = [selected_bg] * len(sample_data)\n",
    "            print(f\"\\n✅ Using sample file as background: {selected_bg['filename']} for {mode}\\n\")\n",
    "        else:\n",
    "            background_data = read_h5_data(base_path, bkg_folder, background_keyword, azimuthal=(x_key == \"q\"))\n",
    "            background_data = sorted(background_data, key=lambda x: x['filename'])\n",
    "\n",
    "            if len(sample_data) != len(background_data):\n",
    "                # use first background file for all\n",
    "                print(f\"Sample data length: {len(sample_data)}\")\n",
    "                print(f\"Background data length: {len(background_data)}\")\n",
    "                print(\"Using first background file for all samples.\")\n",
    "                background_data = [background_data[0]] * len(sample_data)\n",
    "                #raise ValueError(f\"Sample and background data lengths do not match for {mode} subtraction.\")\n",
    "\n",
    "        all_results[mode] = process_subtraction(sample_data, background_data, x_key, save_suffix=mode)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# poni file for 2D image creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Base paths and file definitions\n",
    "#base_path = '/Users/akmaurya/Desktop/20240628/in_situ'  \n",
    "\n",
    "\n",
    "base_path_2D = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/'\n",
    "#poni_file = os.path.join(base_path_2D, 'poni/poni_exsitu/LaB6_poni_New_01.poni')\n",
    "poni_file = os.path.join(base_path_2D, 'poni/poni_insitu/LaB6_insitu_01.poni')\n",
    "#mask_file = os.path.join(base_path, 'poni/poni_exsitu/exsitu_mask_new_09.edf')\n",
    "mask_file = os.path.join(base_path_2D, 'poni/poni_insitu/mask_insitu_new_01.edf')\n",
    "output_base_path_2D = base_path_2D  # Base path for the output directory structure\n",
    "\n",
    "ai = pyFAI.load(poni_file) \n",
    "print (f\"ai: {ai}\")\n",
    "\n",
    "\n",
    "from tkinter import font\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyFAI\n",
    "\n",
    "import numpy as np\n",
    "import pyFAI\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_images(image, poni_file, output_folder, tif_file, azm_range, q_range_2D,mode='azimuthal', vmin=None, vmax=None):\n",
    "\n",
    "    # Load poni file to extract calibration parameters\n",
    "    ai = pyFAI.load(poni_file)\n",
    "    pixel_size = ai.pixel1  # Assuming square pixels\n",
    "    detector_distance = ai.dist\n",
    "\n",
    "    #print(ai)\n",
    "\n",
    "    #beamx = 1028.789\n",
    "    #beamy = 1126.865\n",
    "\n",
    "    beamx = ai.getFit2D()['centerX']\n",
    "    beamy = ai.getFit2D()['centerY']\n",
    "\n",
    "\n",
    "    #print(f\"beamx: {beamx}, beamy: {beamy}\")\n",
    "\n",
    "    wavelength = ai.wavelength\n",
    "\n",
    "    # Convert SAXS to q-space coordinates\n",
    "    x_pixels = image.shape[1]\n",
    "    y_pixels = image.shape[0]\n",
    "    x_coords = np.arange(x_pixels) - beamx\n",
    "    y_coords = np.arange(y_pixels) - beamy\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    qx = 1e-9 * 2 * np.pi / wavelength * np.sin(pixel_size * xx / detector_distance)\n",
    "    qy = 1e-9 * 2 * np.pi / wavelength * np.sin(pixel_size * yy / detector_distance)\n",
    "\n",
    "    # Calculate azimuthal angle (in degrees) for each pixel relative to the beam center\n",
    "    azimuthal_angles = np.degrees(np.arctan2(qy, qx))\n",
    "    \n",
    "    # Create a mask for the azimuthal angle range\n",
    "    mask_azm = (azimuthal_angles >= azm_range[0]) & (azimuthal_angles <= azm_range[1])\n",
    "\n",
    "    # Create a mask for the q range\n",
    "    mask_q = (np.sqrt(qx**2 + qy**2) >= q_range_2D[0]) & (np.sqrt(qx**2 + qy**2) <= q_range_2D[1])\n",
    "\n",
    "    # Plot the 2D image with the sector highlighted\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    # set nans to 0\n",
    "    image = np.nan_to_num(image, nan=0.0)\n",
    "    # Normalize the image  with sum of intensity\n",
    "    image = 10000000* (image / np.sum(image))\n",
    "\n",
    "    \n",
    "    #qx = -qx\n",
    "    #qy = -qy  # Invert y-axis for correct orientation\n",
    "    # take log of the intensity \n",
    "\n",
    "    # Apply logarithmic transformation to the intensity values\n",
    "    #image = np.log1p(image)  # log1p is used to handle zero values safely (log(1 + x))\n",
    "\n",
    "    \n",
    "    im_avg = ax.pcolormesh(qx, qy, (image), cmap='jet', vmin=vmin, vmax=vmax, shading='auto', rasterized=True,zorder=0)\n",
    "    ax.invert_yaxis( )\n",
    "    # Correct the labels: flip the sign of qy for display\n",
    "    ax.set_yticks(np.flip(ax.get_yticks()))  # Flip y-ticks\n",
    "        # Change the sign of the y-axis labels\n",
    "    y_ticks = ax.get_yticks()\n",
    "    ax.set_yticklabels([-int(tick) for tick in y_ticks])  # Convert to integers and negate\n",
    "\n",
    "\n",
    "    x_ticks = np.arange(-20, 21, 10)  # Adjust this range as needed\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels([str(abs(tick)) for tick in x_ticks])  # Display only positive integer labels for x-axis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # flip the axis sign up positive\n",
    "\n",
    "    \n",
    "    \n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Overlay the sector by masking the area outside azimuthal range\n",
    "    if mode == 'azimuthal':\n",
    "\n",
    "        #ax.contourf(qx, qy, mask_azm, levels=[0.5, 1], colors='white', alpha=0.3,zorder=1)  # hatches=['//']\n",
    "        # Semi-transparent fill (distinct from background and wedge)\n",
    "        #ax.contourf(qx, qy, mask_azm, levels=[0.5, 1], colors='green', alpha=0.1, zorder=3)\n",
    "        # Dotted black contour edge\n",
    "        ax.contour(qx, qy, mask_azm, levels=[0.5], colors='white', linewidths=1.5,\n",
    "                linestyles='dotted', zorder=4)\n",
    "    \n",
    "    if mode == 'radial':\n",
    "        # Overlay the sector by masking the area outside q range\n",
    "        #ax.contourf(qx, qy, mask_q, levels=[0.5, 1], colors='white', alpha=0.3,zorder=1)  # hatches=['//']\n",
    "\n",
    "        # Semi-transparent fill (distinct from background and wedge)\n",
    "        #ax.contourf(qx, qy, mask_q, levels=[0.5, 1], colors='green', alpha=0.1, zorder=3)\n",
    "\n",
    "        # Dotted black contour edge\n",
    "        ax.contour(qx, qy, mask_q, levels=[0.5], colors='white', linewidths=1.5,\n",
    "                linestyles='dotted', zorder=4)\n",
    "\n",
    "    cbar_avg = fig.colorbar(im_avg, ax=ax, shrink=.8, label='Intensity [a.u.]')\n",
    "    cbar_avg.ax.tick_params(labelsize=20)\n",
    "    cbar_avg.ax.yaxis.label.set_size(20)\n",
    "    \n",
    "    ax.set_xlabel(r\"$q_x$ [nm$^{-1}$]\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$q_y$ [nm$^{-1}$]\", fontsize=20)\n",
    "    ax.set_title(f\"{f'{os.path.splitext(tif_file)[0]}_2D'}\", fontsize=14, y=1.05)\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Save and show the figure\n",
    "    # creat a Figures folder with output_folder\n",
    "    output_folder = os.path.join(output_folder, \"Figures\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # create the folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    # save the figure\n",
    "    plt.savefig(f\"{output_folder}/{f'{os.path.splitext(tif_file)[0]}'}.png\", dpi=300, bbox_inches='tight')\n",
    "    #plt.close()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D image creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from xml.etree.ElementTree import TreeBuilder\n",
    "import fabio\n",
    "import imageio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def extract_folder_and_filenames(selected_files):\n",
    "    \"\"\"\n",
    "    Extracts folder names and filenames based on the selected filenames.\n",
    "    \n",
    "    Folder name is the part after 'thresh1_' up to '_s' in the filename.\n",
    "    Filename is kept as-is.\n",
    "    \n",
    "    Parameters:\n",
    "        selected_files (list of str): List of filenames.\n",
    "        \n",
    "    Returns:\n",
    "        foldernames (list of str): List of folder names extracted.\n",
    "        filenames (list of str): List of filenames (same as input).\n",
    "    \"\"\"\n",
    "    foldernames = []\n",
    "    filenames = []\n",
    "    \n",
    "    for file in selected_files:\n",
    "        try:\n",
    "            # Remove prefix 'thresh1_' and split at '_s'\n",
    "            name_part = file.split('thresh1_')[1]\n",
    "            foldername = name_part.split('_s000')[0]\n",
    "            foldernames.append(foldername)\n",
    "            filenames.append(file)\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Could not process file {file}\")\n",
    "            continue\n",
    "    \n",
    "    return foldernames, filenames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def average_selected_tif_images(base_path_2D, samp_folder, selected_files, bkg_files, poni_file, mask_file, output_base_path_2D, mode, \n",
    "                                vmin=None, vmax=None, q_range_2D=(3, 35), azm_range=(-105, -75),plot=False):\n",
    "    \"\"\"\n",
    "    Average selected .tif images after background subtraction, apply mask, and save averaged image and plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the mask properly\n",
    "    mask = fabio.open(mask_file).data\n",
    "\n",
    "    # Prepare to collect images\n",
    "    image_list = []\n",
    "\n",
    "    # Extract foldernames and filenames separately\n",
    "    foldernames, filenames = extract_folder_and_filenames(selected_files)\n",
    "    bkg_folders, bkg_filenames = extract_folder_and_filenames(bkg_files)\n",
    "\n",
    "    # Track failed files\n",
    "    failed_files = []\n",
    "\n",
    "    # Loop over selected files\n",
    "    for foldername, filename, bkg_folder, bkg_filename in tqdm(zip(foldernames, filenames, bkg_folders, bkg_filenames), total=len(filenames), desc=\"Processing images\"):\n",
    "        print(f\"Processing {foldername}/{filename} (background: {bkg_filename})...\")\n",
    "\n",
    "        tif_file_path = os.path.join(base_path_2D, 'P3HB', foldername, 'Threshold 1', filename)\n",
    "        bkg_file_path = os.path.join(base_path_2D, 'P3HB', bkg_folder, 'Threshold 1', bkg_filename)\n",
    "\n",
    "        try:\n",
    "            img_sample = imageio.v3.imread(tif_file_path)\n",
    "            img_bkg = imageio.v3.imread(bkg_file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {tif_file_path} or {bkg_file_path}: {e}\")\n",
    "            failed_files.append((filename, bkg_filename))\n",
    "            continue\n",
    "\n",
    "        img = img_sample - 0.95*img_bkg  # Subtract background\n",
    "        img[mask == 1] = 0           # Apply mask\n",
    "        image_list.append(img)\n",
    "\n",
    "        if plot:\n",
    "            try:\n",
    "                save_folder = os.path.join(output_base_path_2D, samp_folder, 'Individual_2D')\n",
    "                os.makedirs(save_folder, exist_ok=True)\n",
    "                plot_2d_images(img, poni_file, save_folder, os.path.basename(filename),\n",
    "                               vmin=vmin, vmax=vmax, azm_range=azm_range,q_range_2D=q_range_2D,mode=mode)\n",
    "            except Exception as plot_error:\n",
    "                print(f\"Error plotting {filename}: {plot_error}\")\n",
    "\n",
    "    # After loop: average the images\n",
    "    if image_list:\n",
    "        avg_image = np.mean(image_list, axis=0)\n",
    "        avg_image[mask == 1] = 0  # Apply mask again just to be sure\n",
    "\n",
    "        avg_save_folder = os.path.join(output_base_path_2D, samp_folder, 'Average_2D')\n",
    "        os.makedirs(avg_save_folder, exist_ok=True)\n",
    "\n",
    "        avg_filename = f'{foldername}_{mode}_average.tiff'\n",
    "        avg_save_path = os.path.join(avg_save_folder, avg_filename)\n",
    "\n",
    "        imageio.imwrite(avg_save_path, avg_image.astype(np.float32))\n",
    "        print(f\"Averaged image saved at {avg_save_path}\")\n",
    "\n",
    "        if plot:\n",
    "            try:\n",
    "                plot_2d_images(avg_image, poni_file, avg_save_folder, f'{foldername}_{mode}_average.png',\n",
    "                               vmin=vmin, vmax=vmax, azm_range=azm_range,q_range_2D=q_range_2D,mode=mode)\n",
    "            except Exception as plot_avg_error:\n",
    "                print(f\"Error plotting average image: {plot_avg_error}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No images were collected for averaging.\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(f\"Warning: {len(failed_files)} files failed to load:\")\n",
    "        for sample, bkg in failed_files:\n",
    "            print(f\"  Sample: {sample} | Background: {bkg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insitu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/OneD_integrated_WAXS_01/insitu/'\n",
    "samp_folder = 'LDPE'\n",
    "match_partial = True\n",
    "scaling_factor = 1.0\n",
    "plot = True\n",
    "\n",
    "# List of (keyword, background_filename, subtract_azimuthal, subtract_chi, use_sample_as_background, bkg_folder, background_keyword)\n",
    "samples = [\n",
    "\n",
    "   #('Run33_f_P3HB_07_50rpm_80C_01', 's00000', True, True, True,'', ''),\n",
    "\n",
    "    \n",
    "    ('insitu_Run2_LDPE_5mmmin_01_Scan00001', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "    ('insitu_Run16_LDPE_film_3mmmin_01_Scan00001', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "]\n",
    "\n",
    "for keyword, background_filename, subtract_azimuthal, subtract_chi, use_sample_as_background, bkg_folder, background_keyword in samples:\n",
    "    subtract_background_v2(\n",
    "        base_path=base_path,\n",
    "        samp_folder=samp_folder,\n",
    "        bkg_folder=bkg_folder,\n",
    "        keyword=keyword,\n",
    "        background_keyword=background_keyword,\n",
    "        subtract_azimuthal=subtract_azimuthal,\n",
    "        subtract_chi=subtract_chi,\n",
    "        scaling_factor=scaling_factor,\n",
    "        plot=plot,\n",
    "        use_sample_as_background=use_sample_as_background,\n",
    "        background_filename=background_filename,\n",
    "        match_partial=match_partial\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/OneD_integrated_WAXS_01/insitu/'\n",
    "samp_folder = 'PHPD'\n",
    "match_partial = True\n",
    "scaling_factor = 1.0\n",
    "plot = False\n",
    "\n",
    "# List of (keyword, background_filename, subtract_azimuthal, subtract_chi, use_sample_as_background, bkg_folder, background_keyword)\n",
    "samples = [\n",
    "    #heated\n",
    "    ##('Run32_f_PET_10_50rpm_3x_cold_draw__120C_01', 's00000', True, True, True,'', ''),  # scaling = 0.93\n",
    " \n",
    "    ('insitu_Run6_PHPD_film_base_5mmmin_01', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "    ('insitu_Run8_PHPD_film_base_3mmmin_01', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "    ('insitu_Run9_PHPD_MBfilm_3mmmin_01', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "    \n",
    "]\n",
    "\n",
    "#keywords= ['cryomill_PET_powder_01_Scan00011','Run17_rep_f_PET_02_50rpm_as_spun_01_Scan00001','Run18_f_PET_03_100rpm_as_spun_01_Scan00001','Run19_f_PET_04_200rpm_as_spun_01_Scan00001','Run20_f_PET_05_400rpm_as_spun_01_Scan00001','Run21_f_PET_06_600rpm_as_spun_01_Scan00001']\n",
    "\n",
    "\n",
    "\n",
    "for keyword, background_filename, subtract_azimuthal, subtract_chi, use_sample_as_background, bkg_folder, background_keyword in samples:\n",
    "    subtract_background_v2(\n",
    "        base_path=base_path,\n",
    "        samp_folder=samp_folder,\n",
    "        bkg_folder=bkg_folder,\n",
    "        keyword=keyword,\n",
    "        background_keyword=background_keyword,\n",
    "        subtract_azimuthal=subtract_azimuthal,\n",
    "        subtract_chi=subtract_chi,\n",
    "        scaling_factor=scaling_factor,\n",
    "        plot=plot,\n",
    "        use_sample_as_background=use_sample_as_background,\n",
    "        background_filename=background_filename,\n",
    "        match_partial=match_partial\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5HV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/OneD_integrated_WAXS_01/insitu/'\n",
    "samp_folder = 'P5HV'\n",
    "match_partial = True\n",
    "scaling_factor = 1.0\n",
    "plot = False\n",
    "\n",
    "# List of (keyword, background_filename, subtract_azimuthal, subtract_chi, use_sample_as_background, bkg_folder, background_keyword)\n",
    "samples = [\n",
    "    #('Run29_f_PLLA_08_50rpm_80C_tension_01', 's00035', True, True, True,'', ''),\n",
    "\n",
    "\n",
    "    ('insitu_Run10_P5HV_film_3mmmin_01', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "    ('insitu_Run14_P5HV_film_3mmmin_01', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "    ('insitu_Run15_P5HV_film_3mmmin_01', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "    ('insitu_Run17_P5HV_film_5mmmin_01', '', True, True, False, 'Air', 'insitu_Run11_air_with_TS600_01'),\n",
    "]\n",
    "\n",
    "\n",
    "for keyword, background_filename, subtract_azimuthal, subtract_chi, use_sample_as_background, bkg_folder, background_keyword in samples:\n",
    "    subtract_background_v2(\n",
    "        base_path=base_path,\n",
    "        samp_folder=samp_folder,\n",
    "        bkg_folder=bkg_folder,\n",
    "        keyword=keyword,\n",
    "        background_keyword=background_keyword,\n",
    "        subtract_azimuthal=subtract_azimuthal,\n",
    "        subtract_chi=subtract_chi,\n",
    "        scaling_factor=scaling_factor,\n",
    "        plot=plot,\n",
    "        use_sample_as_background=use_sample_as_background,\n",
    "        background_filename=background_filename,\n",
    "        match_partial=match_partial\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot 1D and 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_scan_number(filename):\n",
    "    \"\"\"Extract scan number from filename like ...Scan00010_....tif → 10\"\"\"\n",
    "    match = re.search(r'Scan(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def plot_subtracted_data(base_path, samp_folder, keyword, poni_file, mask_file,\n",
    "                         vmin=0, vmax=15, mode=\"azimuthal\", pattern_name=\"subtracted\",\n",
    "                         filter_intensity=1000, sort_by='scan', top_n=None,\n",
    "                         q_range_2D=(3, 35), azm_range=(-180, 0), plot_2D=False):\n",
    "    subtracted_dir = os.path.join(base_path, 'Subtracted_Data')\n",
    "    modes = ['azimuthal', 'radial'] if mode == \"both\" else [mode]\n",
    "\n",
    "    # Dictionaries to store entries keyed by filename\n",
    "    azimuthal_entries = {}\n",
    "    radial_entries = {}\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    for current_mode in modes:\n",
    "        search_pattern = os.path.join(subtracted_dir, f\"{pattern_name}_*{keyword}*_{current_mode}_data.h5\")\n",
    "        matching_files = glob.glob(search_pattern)\n",
    "        if not matching_files:\n",
    "            raise FileNotFoundError(f\"No file matching pattern: {search_pattern}\")\n",
    "\n",
    "        print(f\"\\nProcessing mode: {current_mode}\")\n",
    "        for subtracted_file in matching_files:\n",
    "            print(f\"Reading: {subtracted_file}\")\n",
    "            with h5py.File(subtracted_file, 'r') as hf:\n",
    "                keys = sorted(\n",
    "                    hf.keys(),\n",
    "                    key=lambda k: hf[k][\"filename\"][()].decode(\"utf-8\")\n",
    "                    if isinstance(hf[k][\"filename\"][()], bytes)\n",
    "                    else hf[k][\"filename\"][()]\n",
    "                )\n",
    "                for key in keys:\n",
    "                    group = hf[key]\n",
    "                    x = np.array(group[\"x\"])\n",
    "                    I = np.array(group[\"I\"])\n",
    "                    filename = group[\"filename\"][()].decode(\"utf-8\") if isinstance(group[\"filename\"][()], bytes) else group[\"filename\"][()]\n",
    "                    scan_number = extract_scan_number(filename)\n",
    "                    bkg_filename = group[\"background_file\"][()].decode(\"utf-8\") if isinstance(group[\"background_file\"][()], bytes) else group[\"background_file\"][()]\n",
    "                    if scan_number is None:\n",
    "                        continue\n",
    "\n",
    "                    if current_mode == \"azimuthal\":\n",
    "                        mask = (x >= 3) & (x <= 45)\n",
    "                        x_masked = x[mask]\n",
    "                        I_masked = I[mask]\n",
    "                        focus_mask = (x_masked >= 5) & (x_masked <= 20)\n",
    "                    else:  # radial\n",
    "                        x_masked = x\n",
    "                        I_masked = I\n",
    "                        focus_mask = (x_masked >= -100) & (x_masked <= -80)\n",
    "\n",
    "                    I_focus = I_masked[focus_mask]\n",
    "                    peak_intensity = np.max(I_focus)\n",
    "                    entry = (peak_intensity, x_masked, I_masked, filename, scan_number, bkg_filename)\n",
    "\n",
    "                    plt.plot(x_masked, I_masked, 'o-', markersize=1)\n",
    "                    #print(mode)\n",
    "                    set_plot_style(plt.gca(), 20, \"q [1/nm]\" if current_mode == \"azimuthal\" else \"Chi [degrees]\", \"Intensity [a.u.]\")\n",
    "                    plt.title(f\"{filename}\")\n",
    "\n",
    "                    if current_mode == \"azimuthal\":\n",
    "                        azimuthal_entries[filename] = entry\n",
    "                    else:\n",
    "                        radial_entries[filename] = entry\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(subtracted_dir, f\"{pattern_name}_{keyword}_{mode}_scan.png\"), dpi=300)\n",
    "                plt.show()\n",
    "\n",
    "    # Match filenames present in both azimuthal and radial\n",
    "    common_filenames = set(azimuthal_entries.keys()) & set(radial_entries.keys())\n",
    "\n",
    "    filtered_data = {\"azimuthal\": [], \"radial\": []}\n",
    "    for fname in common_filenames:\n",
    "        az_entry = azimuthal_entries[fname]\n",
    "        rad_entry = radial_entries[fname]\n",
    "\n",
    "        az_high = np.max(az_entry[2]) > filter_intensity\n",
    "        #rad_high = np.max(rad_entry[2]) > filter_intensity\n",
    "        if az_high:\n",
    "            print(f\"Skipping {fname} due to high intensity in both modes.\")\n",
    "            continue\n",
    "\n",
    "        filtered_data[\"azimuthal\"].append(az_entry)\n",
    "        filtered_data[\"radial\"].append(rad_entry)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    all_mode_data = filtered_data\n",
    "\n",
    "    # Selection based on sorting strategy\n",
    "    selected_data = {}\n",
    "    for current_mode in modes:\n",
    "        data = all_mode_data[current_mode]\n",
    "        if sort_by == 'scan':\n",
    "            scan_data = defaultdict(list)\n",
    "            for entry in data:\n",
    "                scan_data[entry[4]].append(entry)\n",
    "            selected = [max(entries, key=lambda t: t[0]) for entries in scan_data.values()]\n",
    "            sorted_data = sorted(selected, key=lambda t: t[4])\n",
    "        elif sort_by == 'all':\n",
    "            sorted_data = sorted(data, key=lambda t: t[0], reverse=True)\n",
    "        else:\n",
    "            raise ValueError(\"sort_by must be 'scan' or 'all'\")\n",
    "\n",
    "        if top_n is not None:\n",
    "            sorted_data = sorted_data[:top_n]\n",
    "\n",
    "        selected_data[current_mode] = sorted_data\n",
    "\n",
    "    # Use the same filenames for both radial and azimuthal if mode == \"both\"\n",
    "    if mode == \"both\":\n",
    "        selected_files = [t[3] for t in selected_data['azimuthal']]\n",
    "        bkg_files = [t[5] for t in selected_data['azimuthal']]\n",
    "    else:\n",
    "        selected_files = [t[3] for t in selected_data[mode]]\n",
    "        bkg_files = [t[5] for t in selected_data[mode]]\n",
    "\n",
    "    print(f\"{'Sample File':<50}\")\n",
    "    print('-' * 80)\n",
    "    for sample in (selected_files):\n",
    "        print(f\"{sample:<50}\")\n",
    "\n",
    "    base_path_2D = os.path.dirname(os.path.dirname(os.path.dirname(base_path)))\n",
    "    output_folder_2D = os.path.join(base_path, 'Subtracted_Data', 'Averaged', '2DFigures')\n",
    "    os.makedirs(output_folder_2D, exist_ok=True)\n",
    "\n",
    "    for current_mode in modes:\n",
    "        average_selected_tif_images(\n",
    "            base_path_2D=base_path_2D,\n",
    "            samp_folder=samp_folder,\n",
    "            selected_files=selected_files,\n",
    "            bkg_files=bkg_files,\n",
    "            poni_file=poni_file,\n",
    "            mask_file=mask_file,\n",
    "            output_base_path_2D=output_folder_2D,\n",
    "            vmin=vmin, vmax=vmax,\n",
    "            mode=current_mode,\n",
    "            q_range_2D=q_range_2D,\n",
    "            azm_range=azm_range,\n",
    "            plot=plot_2D\n",
    "        )\n",
    "\n",
    "        # Plotting 1D data\n",
    "        data = selected_data[current_mode]\n",
    "        x_ref = data[0][1]\n",
    "        all_intensities = []\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for i, (peak, x, I, filename, _, bkg) in enumerate(data):\n",
    "            I_norm = I.copy()  # Make a copy so you don't overwrite I\n",
    "\n",
    "            '''\n",
    "            if current_mode == 'radial':\n",
    "            \n",
    "                peak_index = np.argmax(I_norm)  # Find the index of the peak in the original array\n",
    "                peak_x = x[peak_index]     # Get the corresponding x-value at the peak\n",
    "\n",
    "                if I_norm[peak_index] > 2000:\n",
    "                    x = x - peak_x - 90\n",
    "                    #x_ref = x\n",
    "            '''\n",
    "\n",
    "            all_intensities.append(I_norm)\n",
    "            plt.plot(x, I_norm, '-', alpha=0.4, label=os.path.basename(filename) if i < 10 else \"\")\n",
    "\n",
    "        avg_I = np.mean(all_intensities, axis=0)\n",
    "        plt.plot(x_ref, avg_I, 'k-', linewidth=2.5, label='Average')\n",
    "\n",
    "        filename_avg = os.path.basename(filename).replace('.h5', '_avg.h5')\n",
    "\n",
    "        set_plot_style(plt.gca(), 20, \"q [1/nm]\" if current_mode == \"azimuthal\" else \"Chi [degrees]\", \"Normalized Intensity [a.u.]\")\n",
    "\n",
    "        output_folder = os.path.join(base_path, 'Subtracted_Data', 'Averaged', 'Filtered')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_folder_avg = os.path.join(base_path, 'Subtracted_Data', 'Averaged')\n",
    "\n",
    "        txt_file = os.path.join(output_folder_avg, f'subtracted_avg_{keyword}_{current_mode}_sortby_{sort_by}_top{len(data) if top_n else \"all\"}.txt')\n",
    "        h5_file = os.path.join(output_folder_avg, f'subtracted_avg_{keyword}_{current_mode}_sortby_{sort_by}_top{len(data) if top_n else \"all\"}.h5')\n",
    "        png_file = os.path.join(output_folder, f'Subtracted_{keyword}_{current_mode}_sortby_{sort_by}_top{len(data) if top_n else \"all\"}.png')\n",
    "        sorted_h5_file = os.path.join(output_folder, f'subtracted_{keyword}_{current_mode}_sortby_{sort_by}_top{len(data) if top_n else \"all\"}.h5')\n",
    "\n",
    "        np.savetxt(txt_file, np.column_stack((x_ref, avg_I)), header=\"q [1/nm] \\t Avg Intensity [a.u.]\")\n",
    "        print(f\"Saved averaged data to {txt_file}\")\n",
    "        write_sorted_data_to_h5([(0, x_ref, avg_I, filename_avg, 0)], h5_file, mode=current_mode)\n",
    "        write_sorted_data_to_h5(data, sorted_h5_file, mode=current_mode)\n",
    "        print(f\"Saved sorted data to {sorted_h5_file}\")\n",
    "\n",
    "        plt.title(f'Averaged_{keyword}_{current_mode}_sortby_{sort_by}_top{len(data) if top_n else \"all\"}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(png_file, dpi=300)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_subtracted_data_all(base_path, samp_folder, keyword, poni_file, mask_file,\n",
    "                             vmin=0, vmax=15, mode=\"azimuthal\", pattern_name=\"subtracted\",\n",
    "                             q_range_2D=(3, 35), azm_range=(-180, 0), plot_2D=False):\n",
    "    subtracted_dir = os.path.join(base_path, 'Subtracted_Data')\n",
    "    modes = ['azimuthal', 'radial'] if mode == \"both\" else [mode]\n",
    "\n",
    "    for current_mode in modes:\n",
    "        search_pattern = os.path.join(subtracted_dir, f\"{pattern_name}_*{keyword}*_{current_mode}_data.h5\")\n",
    "        matching_files = glob.glob(search_pattern)\n",
    "        if not matching_files:\n",
    "            raise FileNotFoundError(f\"No file matching pattern: {search_pattern}\")\n",
    "\n",
    "        print(f\"\\nProcessing mode: {current_mode}\")\n",
    "        all_data = []\n",
    "\n",
    "        # For surface plot data collection\n",
    "        intensity_matrix = []\n",
    "        x_axis = None\n",
    "\n",
    "        # Set up plot\n",
    "        plt.figure(figsize=(6, 4))\n",
    "\n",
    "        # Colormap setup for line plot\n",
    "        num_colors = sum(len(h5py.File(f, 'r').keys()) for f in matching_files)\n",
    "        cmap = cm.get_cmap('viridis', num_colors)\n",
    "        norm = mcolors.Normalize(vmin=0, vmax=num_colors - 1)\n",
    "\n",
    "        color_idx = 0\n",
    "\n",
    "        for subtracted_file in matching_files:\n",
    "            print(f\"Reading: {subtracted_file}\")\n",
    "            with h5py.File(subtracted_file, 'r') as hf:\n",
    "                keys = sorted(\n",
    "                    hf.keys(),\n",
    "                    key=lambda k: hf[k][\"filename\"][()].decode(\"utf-8\")\n",
    "                    if isinstance(hf[k][\"filename\"][()], bytes)\n",
    "                    else hf[k][\"filename\"][()]\n",
    "                )\n",
    "                for key in keys:\n",
    "                    group = hf[key]\n",
    "                    x = np.array(group[\"x\"])\n",
    "                    I = np.array(group[\"I\"])\n",
    "                    filename = group[\"filename\"][()].decode(\"utf-8\") if isinstance(group[\"filename\"][()], bytes) else group[\"filename\"][()]\n",
    "                    bkg_filename = group[\"background_file\"][()].decode(\"utf-8\") if isinstance(group[\"background_file\"][()], bytes) else group[\"background_file\"][()]\n",
    "\n",
    "                    if current_mode == \"azimuthal\":\n",
    "                        mask = (x >= 0) & (x <= 45)\n",
    "                        x_masked = x[mask]\n",
    "                        I_masked = I[mask]\n",
    "                    else:\n",
    "                        x_masked = x\n",
    "                        I_masked = I\n",
    "\n",
    "                    all_data.append((x_masked, I_masked, filename, bkg_filename))\n",
    "                    intensity_matrix.append(I_masked)\n",
    "                    if x_axis is None:\n",
    "                        x_axis = x_masked  # Use x-axis from the first entry\n",
    "\n",
    "                    color = cmap(norm(color_idx))\n",
    "                    plt.plot(x_masked, I_masked, '-', alpha=0.8, color=color, label=os.path.basename(filename))\n",
    "                    color_idx += 1\n",
    "\n",
    "        # Line plot\n",
    "        set_plot_style(plt.gca(), 20, \"q [1/nm]\" if current_mode == \"azimuthal\" else \"Chi [degrees]\", \"Intensity [a.u.]\")\n",
    "        plt.title(f\"{keyword} - {current_mode} - All Scans\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(subtracted_dir, f\"{pattern_name}_{keyword}_{current_mode}_ALL.png\"), dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        # Surface Plot\n",
    "        intensity_matrix = np.array(intensity_matrix)\n",
    "        if intensity_matrix.ndim == 2:\n",
    "            plt.figure(figsize=(7, 5))\n",
    "            extent = [x_axis[0], x_axis[-1], 0, intensity_matrix.shape[0]]\n",
    "            aspect = 'auto'\n",
    "\n",
    "            plt.imshow(intensity_matrix, extent=extent, origin='lower',\n",
    "                       aspect=aspect, cmap='viridis', vmin=np.min(intensity_matrix),\n",
    "                       vmax=np.max(intensity_matrix))\n",
    "            plt.colorbar(label=\"Intensity [a.u.]\")\n",
    "            plt.xlabel(\"q [1/nm]\" if current_mode == \"azimuthal\" else \"Chi [degrees]\")\n",
    "            plt.ylabel(\"Scan index\")\n",
    "            plt.title(f\"Surface Plot - {keyword} - {current_mode}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(subtracted_dir, f\"{pattern_name}_{keyword}_{current_mode}_SURFACE.png\"), dpi=300)\n",
    "            plt.show()\n",
    "\n",
    "        # Optional 2D plotting (external function)\n",
    "        if plot_2D:\n",
    "            base_path_2D = os.path.dirname(os.path.dirname(os.path.dirname(base_path)))\n",
    "            output_folder_2D = os.path.join(base_path, 'Subtracted_Data', '2DFigures')\n",
    "            os.makedirs(output_folder_2D, exist_ok=True)\n",
    "\n",
    "            selected_files = [t[2] for t in all_data]\n",
    "            bkg_files = [t[3] for t in all_data]\n",
    "\n",
    "            average_selected_tif_images(\n",
    "                base_path_2D=base_path_2D,\n",
    "                samp_folder=samp_folder,\n",
    "                selected_files=selected_files,\n",
    "                bkg_files=bkg_files,\n",
    "                poni_file=poni_file,\n",
    "                mask_file=mask_file,\n",
    "                output_base_path_2D=output_folder_2D,\n",
    "                vmin=vmin, vmax=vmax,\n",
    "                mode=current_mode,\n",
    "                q_range_2D=q_range_2D,\n",
    "                azm_range=azm_range,\n",
    "                plot=True\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P3HB dataset processing\n",
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/OneD_integrated_WAXS_01/insitu'\n",
    "samp_folder = 'LDPE'\n",
    "\n",
    "print(f\"Processing {samp_folder}...\")\n",
    "\n",
    "samples = [\n",
    "    (\"insitu_Run2_LDPE_5mmmin_01_Scan00001\"),\n",
    "    (\"insitu_Run16_LDPE_film_3mmmin_01_Scan00001\"),\n",
    "\n",
    "]\n",
    "for keyword in samples:\n",
    "    plot_subtracted_data_all(\n",
    "        base_path=base_path,\n",
    "        samp_folder=samp_folder,\n",
    "        poni_file=poni_file,\n",
    "        mask_file=mask_file,\n",
    "        keyword=keyword,\n",
    "        mode=\"both\",\n",
    "        \n",
    "        q_range_2D=(14, 16), \n",
    "        azm_range=(-45, 45),\n",
    "        \n",
    "        plot_2D=True\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P3HB dataset processing\n",
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/OneD_integrated_WAXS_01/insitu'\n",
    "samp_folder = 'PHPD'\n",
    "\n",
    "print(f\"Processing {samp_folder}...\")\n",
    "\n",
    "samples = [\n",
    "    'insitu_Run6_PHPD_film_base_5mmmin_01', \n",
    "    'insitu_Run8_PHPD_film_base_3mmmin_01',\n",
    "    'insitu_Run9_PHPD_MBfilm_3mmmin_01', \n",
    "\n",
    "]\n",
    "for keyword in samples:\n",
    "    plot_subtracted_data_all(\n",
    "        base_path=base_path,\n",
    "        samp_folder=samp_folder,\n",
    "        poni_file=poni_file,\n",
    "        mask_file=mask_file,\n",
    "        keyword=keyword,\n",
    "        mode=\"both\",\n",
    "        \n",
    "        q_range_2D=(14, 16), \n",
    "        azm_range=(-45, 45),\n",
    "        \n",
    "        plot_2D=True\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5HV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P3HB dataset processing\n",
    "base_path = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/OneD_integrated_WAXS_01/insitu'\n",
    "samp_folder = 'P5HV'\n",
    "\n",
    "print(f\"Processing {samp_folder}...\")\n",
    "\n",
    "samples = [\n",
    "     'insitu_Run10_P5HV_film_3mmmin_01',\n",
    "    'insitu_Run14_P5HV_film_3mmmin_01', \n",
    "    'insitu_Run15_P5HV_film_3mmmin_01',\n",
    "    'insitu_Run17_P5HV_film_5mmmin_01', \n",
    "\n",
    "]\n",
    "for keyword in samples:\n",
    "    plot_subtracted_data_all(\n",
    "        base_path=base_path,\n",
    "        samp_folder=samp_folder,\n",
    "        poni_file=poni_file,\n",
    "        mask_file=mask_file,\n",
    "        keyword=keyword,\n",
    "        mode=\"both\",\n",
    "        \n",
    "        q_range_2D=(14, 16), \n",
    "        azm_range=(-45, 45),\n",
    "        \n",
    "        plot_2D=True\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting- 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azimuthal integration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fnmatch\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "def plot_filtered_files(folder_path, patterns, xlim=(4, 20), colormap_name='viridis'):\n",
    "    \"\"\"\n",
    "    Plot data from text files in a given folder, filtered by wildcard patterns in filenames.\n",
    "    All files are plotted using a smooth color transition from a colormap.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing the .txt files\n",
    "    - patterns (list): List of wildcard patterns to filter filenames\n",
    "    - xlim (tuple): x-axis limits for the plot\n",
    "    - colormap_name (str): Name of matplotlib colormap\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6.5, 6), dpi=300)\n",
    "    plt.clf()\n",
    "    plt.ion()\n",
    "\n",
    "    matched_files = []\n",
    "    for pattern in patterns:\n",
    "        for f in sorted(os.listdir(folder_path)):\n",
    "            if (\n",
    "                f.endswith('.txt') and\n",
    "                'azimuthal' in f and\n",
    "                not f.startswith('._') and\n",
    "                fnmatch.fnmatch(f, pattern)\n",
    "            ):\n",
    "                matched_files.append(f)\n",
    "\n",
    "    # Apply color transition using the colormap\n",
    "    cmap = get_cmap(colormap_name)\n",
    "    colors = cmap(np.linspace(0, 1, len(matched_files)))\n",
    "\n",
    "    for idx, filename in enumerate(matched_files):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            q_values = []\n",
    "            intensity_values = []\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if (\n",
    "                        line.startswith('#') or\n",
    "                        line.strip() == '' or\n",
    "                        'dtype' in line or\n",
    "                        'q:' in line or\n",
    "                        'intensity:' in line\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        q_values.append(float(parts[0]))\n",
    "                        intensity_values.append(float(parts[1]))\n",
    "\n",
    "            # Create label\n",
    "            label = filename.split('.')[0]\n",
    "            label = label.replace('thresh1_', '').split('_Scan')[0]\n",
    "            label = label.split('f_')[-1] if 'f_' in label else label\n",
    "            label = label.split('_azimuthal')[0] if 'azimuthal' in label else label\n",
    "            label = label.replace('P3HB', 'PHA-3B')\n",
    "            label = label.replace('rep_', '').replace('_01', '')\n",
    "            label = label.replace('subtracted_avg_', '')\n",
    "\n",
    "            # Normalize intensity\n",
    "            intensity_values = np.array(intensity_values)\n",
    "            intensity_values = 10000 * intensity_values / np.sum(intensity_values)\n",
    "\n",
    "            plt.plot(q_values, intensity_values, '-o', label=label, markersize=2, color=colors[idx])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")\n",
    "\n",
    "    plt.xlabel('q [nm⁻¹]')\n",
    "    plt.ylabel('Intensity [a.u.]')\n",
    "    set_plot_style(plt.gca(), 20, 'q [nm⁻¹]', 'Intensity [a.u.]')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(0, )\n",
    "    plt.legend(loc='upper right', fontsize=12, frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/OneD_integrated_WAXS_01/P3HB/Subtracted_Data/Averaged'\n",
    "#patterns = ['*PLLA_*s00010*', '*PLLA*_s000014*']\n",
    "patterns = ['*cryomill_P3HB_powder*_scan*',\n",
    " '*avg_P3HB_*_50RPM*scan*',\n",
    " '*avg_P3HB_*_100RPM*scan*',\n",
    " '*avg_P3HB_*_200RPM*scan*',\n",
    " '*avg_P3HB_*_400RPM*scan*',\n",
    " '*avg_P3HB_*_600RPM*scan*'\n",
    "]\n",
    "#avg_P3HB_03_100RPM_rep\n",
    "\n",
    "plot_filtered_files(folder, patterns, xlim=(5, 20))\n",
    "\n",
    "\n",
    "patterns = ['*cryomill_PLLA_powder*',\n",
    " '*PLLA_08_50rpm_80C_tension*',\n",
    "    '*PLLA_07_50rpm_80C_free*',]\n",
    "plot_filtered_files(folder, patterns, xlim=(11, 13))\n",
    "\n",
    "patterns = ['*cryomill*PLLA*',\n",
    "    '*PLLA*50rpm*as_spun*',\n",
    "    '*PLLA*100rpm*as_spun*',\n",
    "    '*PLLA*200rpm*as_spun*',\n",
    "    '*PLLA*400rpm*as_spun*',\n",
    "    '*PLLA*600rpm*as_spun*',\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "plot_filtered_files(folder, patterns, xlim=(11, 13))\n",
    "\n",
    "\n",
    "patterns = ['*cryomill_P3HB_powder*',\n",
    " '*P3HB_07_50rpm_80C_*scan*',\n",
    "    '*P3HB_08_50rpm_3xcold_draw_*scan*',\n",
    "    '*P3HB_09_50rpm_3xcolddraw_80C_*scan*',\n",
    "\n",
    "]\n",
    "plot_filtered_files(folder, patterns, xlim=(5, 20))\n",
    "\n",
    "\n",
    "patterns = ['*cryomill_PET_powder*',\n",
    " '*avg_PET_*_50rpm*',\n",
    " '*avg_PET_*_100rpm*',\n",
    " '*avg_PET_*_200rpm*',\n",
    " '*avg_PET_*_400rpm*',\n",
    " '*avg_PET_*_600rpm*',\n",
    "    '*PET_07_400rpm_3xcolddraw*'\n",
    " \n",
    "]\n",
    "plot_filtered_files(folder, patterns, xlim=(5, 25))\n",
    "\n",
    "\n",
    "patterns = ['*cryomill_PET_powder*',\n",
    " '*PET_10_50rpm_3x_cold_draw*',\n",
    " '*PET_09_50rpm_3x_cold_draw*',\n",
    " '*PET_08_50rpm_120C*',\n",
    " '*PET_07_400rpm_3xcolddraw*',\n",
    "]\n",
    "plot_filtered_files(folder, patterns, xlim=(5, 25))\n",
    "\n",
    "patterns = [#'*cryomill_PET_powder*',\n",
    " '*f_PET_Fiber1_Ozge_rep_01*',\n",
    " #'*f_PET_Fiber1_Ozge_01*',\n",
    " '*f_PET_Fiber2_Ozge_01*',\n",
    " '*f_PET_Fiber3_Ozge_01*',\n",
    " '*f_PET_Fiber1_Ozge_10min_methanolysis_01*',\n",
    "]\n",
    "\n",
    "\n",
    "plot_filtered_files(folder, patterns, xlim=(5, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def read_and_plot_chi_intensity(folder_path, patterns, normalize=False, xlim=(-180, -0), ylim=None, figsize=(6.5, 6), dpi=300):\n",
    "    \"\"\"\n",
    "    Plot Chi vs Intensity data using Viridis colormap and styled formatting.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Folder containing .txt files\n",
    "    - patterns (list of str): Wildcard patterns to match filenames\n",
    "    - normalize (bool): Normalize intensity if True\n",
    "    - xlim (tuple): x-axis limits\n",
    "    - ylim (tuple or None): y-axis limits\n",
    "    - figsize (tuple): Figure size\n",
    "    - dpi (int): Plot DPI\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    plt.clf()\n",
    "    plt.ion()\n",
    "\n",
    "    all_labels = []\n",
    "    all_data = []\n",
    "\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if (\n",
    "            filename.endswith('.txt')\n",
    "            and 'radial' in filename\n",
    "            and not filename.startswith('._')\n",
    "            and any(fnmatch.fnmatch(filename, pattern) for pattern in patterns)\n",
    "        ):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            chi, intensity = [], []\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line in file:\n",
    "                        if line.strip() == '' or line.startswith('#'):\n",
    "                            continue\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 2:\n",
    "                            chi.append(float(parts[0]))\n",
    "                            intensity.append(float(parts[1]))\n",
    "\n",
    "                if not chi:\n",
    "                    print(f\"No data in {filename}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                chi = np.array(chi)\n",
    "                intensity = np.array(intensity)\n",
    "\n",
    "\n",
    "\n",
    "                # subtract a constant   basleine bu taking average of 10 lowest intesity values in the chi range of -120 to -20\n",
    "                mask = (chi >= -180) & (chi <= -0)\n",
    "                intensity = intensity[mask]\n",
    "                chi = chi[mask]\n",
    "                #if len(intensity[mask]) > 10:\n",
    "                    #baseline = np.mean(intensity[mask][:10])\n",
    "                    #intensity -= baseline\n",
    "                #intensity[intensity < 0] = 0  # Set negative values to zero\n",
    "\n",
    "                # Exclude bad chi range\n",
    "\n",
    "                \n",
    "\n",
    "                if normalize:\n",
    "                    intensity = 10000 * intensity / np.sum(intensity)\n",
    "                \n",
    "                mask = (chi <= -80) | (chi >= -70)\n",
    "                chi = chi[mask]\n",
    "                intensity = intensity[mask]\n",
    "\n",
    "                label = filename.split('.')[0].replace('thresh1_', '')\n",
    "                label = label.split('_radial')[0]\n",
    "                label = label.split('_avg_')[1] if '_avg_' in label else label\n",
    "                label = label.split('_01_Scan')[0]\n",
    "                label = label.replace('rep_', '').replace('_01', '')\n",
    "                if 'f_' in label:\n",
    "                    label = label.split('f_')[1]\n",
    "                if 'P3HB' in label:\n",
    "                    label = label.replace('P3HB', 'PHA-3B')\n",
    "\n",
    "                all_labels.append(label)\n",
    "                all_data.append((chi, intensity))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {file_path}: {e}\")\n",
    "    \n",
    "    cmap = cm.get_cmap('viridis', len(all_data))  # Viridis colormap\n",
    "\n",
    "    for i, (label, (chi, intensity)) in enumerate(zip(all_labels, all_data)):\n",
    "        color = cmap(i)\n",
    "        plt.plot(chi, intensity, '-o', label=label, color=color, markersize=2)\n",
    "\n",
    "    plt.xlim(*xlim)\n",
    "    if ylim:\n",
    "        plt.ylim(*ylim)\n",
    "\n",
    "    plt.xlabel('Chi [degrees]')\n",
    "    plt.ylabel('Intensity [a.u.]')\n",
    "    set_plot_style(plt.gca(), 20, 'Chi [degrees]', 'Intensity [a.u.]')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/OneD_integrated_WAXS_01/P3HB/Subtracted_Data/Averaged'\n",
    "#patterns = ['*PLLA_*s00010*', '*PLLA*_s000014*']\n",
    "patterns = ['*cryomill_P3HB_powder*_scan*',\n",
    " '*avg_P3HB_*_50RPM*scan*',\n",
    " '*avg_P3HB_*_100RPM*scan*',\n",
    " '*avg_P3HB_*_200RPM*scan*',\n",
    " '*avg_P3HB_*_400RPM*scan*',\n",
    " '*avg_P3HB_*_600RPM*scan*'\n",
    "]\n",
    "#avg_P3HB_03_100RPM_rep\n",
    "\n",
    "read_and_plot_chi_intensity(folder, patterns)\n",
    "\n",
    "\n",
    "patterns = ['*cryomill_PLLA_powder*_scan*',\n",
    " '*PLLA_08_50rpm_80C_tension*',\n",
    "    '*PLLA_07_50rpm_80C_free*',]\n",
    "read_and_plot_chi_intensity(folder, patterns)\n",
    "\n",
    "patterns = [#'*cryomill*PLLA*',\n",
    "    '*PLLA*50rpm*as_spun*',\n",
    "    '*PLLA*100rpm*as_spun*',\n",
    "    '*PLLA*200rpm*as_spun*',\n",
    "    '*PLLA*400rpm*as_spun*',\n",
    "    '*PLLA*600rpm*as_spun*',\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "read_and_plot_chi_intensity(folder, patterns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patterns = ['*cryomill_P3HB_powder*_scan*',\n",
    " '*P3HB_07_50rpm_80C_*scan*',\n",
    "    '*P3HB_08_50rpm_3xcold_draw_*scan*',\n",
    "    '*P3HB_09_50rpm_3xcolddraw_80C_*scan*',\n",
    "\n",
    "]\n",
    "read_and_plot_chi_intensity(folder, patterns)\n",
    "\n",
    "\n",
    "patterns = ['*cryomill_PET_powder*',\n",
    " '*avg_PET_*_50rpm*',\n",
    " '*avg_PET_*_100rpm*',\n",
    " '*avg_PET_*_200rpm*',\n",
    " '*avg_PET_*_400rpm*',\n",
    " '*avg_PET_*_600rpm*'\n",
    "]\n",
    "read_and_plot_chi_intensity(folder, patterns)\n",
    "\n",
    "\n",
    "patterns = ['*cryomill_PET_powder*',\n",
    " '*PET_10_50rpm_3x_cold_draw*',\n",
    " '*PET_09_50rpm_3x_cold_draw*',\n",
    " '*PET_08_50rpm_120C*',\n",
    " #'*PET_07_400rpm_3xcolddraw*',\n",
    "]\n",
    "read_and_plot_chi_intensity(folder, patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial 1D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_plot_chi_intensity(folder_path, patterns, normalize=False):\n",
    "    \"\"\"\n",
    "    Searches for files matching patterns in the folder, reads Chi vs I_radial data, and plots.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing data files\n",
    "    - patterns (list): List of wildcard patterns to match filenames\n",
    "    - normalize (bool): If True, normalize the intensity values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6.5, 6), dpi=300)\n",
    "    plt.clf()\n",
    "    plt.ion()  # Enable interactive mode\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if (\n",
    "            filename.endswith('.txt')\n",
    "            and 'radial' in filename\n",
    "            and not filename.startswith('._')\n",
    "            and any(fnmatch.fnmatch(filename, pattern) for pattern in patterns)\n",
    "        ):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            chi = []\n",
    "            intensity = []\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line in file:\n",
    "                        # Skip header or empty lines\n",
    "                        if line.strip() == '' or line.startswith('#'):\n",
    "                            continue\n",
    "\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 2:\n",
    "                            chi.append(float(parts[0]))\n",
    "                            intensity.append(float(parts[1]))\n",
    "\n",
    "                if not chi:\n",
    "                    print(f\"No data found in {filename}. Skipping.\")\n",
    "                    continue\n",
    "                # mask intesity in given chi mask in -82 to -75 but pass out of of this range\n",
    "                chi = np.array(chi)\n",
    "                intensity = np.array(intensity)\n",
    "                mask = (chi <= -80) | (chi >= -74)\n",
    "                chi = chi[mask]\n",
    "                intensity = intensity[mask]\n",
    "                # Optional normalization\n",
    "                if normalize:\n",
    "                    max_intensity = max(intensity)\n",
    "                    #intensity = [i / max_intensity for i in intensity]\n",
    "\n",
    "                # Normalize intensity by sum of total intensity\n",
    "                intensity = np.array(intensity)\n",
    "                intensity = 10000 * intensity / np.sum(intensity)\n",
    "\n",
    "                # Use part of the filename as label\n",
    "                #label = filename.split('_')[2]  # Adjust index if needed\n",
    "                label = filename.split('.')[0]  # Adjust if needed\n",
    "                print(f\"Filename: {label}\")\n",
    "                label = label.replace('thresh1_', '')\n",
    "                label = label.split('_01_Scan')[0]\n",
    "                if 'f_' in label:\n",
    "                    label = label.split('f_')[1]\n",
    "                \n",
    "                if 'P3HB' in label:\n",
    "                    label = label.replace('P3HB', 'PHA-3B')\n",
    "\n",
    "                if 'rep_' in label:\n",
    "                    label = label.replace('rep_', '')\n",
    "                label = label.replace('_01', '')\n",
    "                \n",
    "                plt.plot(chi, intensity,'-o',label=label, markersize=2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "    plt.xlabel('Chi (degrees)')\n",
    "    plt.ylabel('Intensity (a.u.)')\n",
    "    plt.xlim(-120, -60)\n",
    "    #plt.ylim(2, 10)\n",
    "    set_plot_style(plt.gca(), 20, 'Chi [degrees]', 'Intensity [a.u.]')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "folder = '/Volumes/SSD1/RawData1/Redesigned_Plastics/Dec2024/D_txt/'\n",
    "patterns = [#'*cryomill*PLLA*_s00005*',\n",
    "    '*PLLA*50rpm*s00010*',\n",
    "    '*PLLA*100rpm*s00010*',\n",
    "    '*PLLA*200rpm*s00009*',\n",
    "    '*PLLA*400rpm*s00014*',\n",
    "    '*PLLA*600rpm*s00010*',\n",
    "    \n",
    "]\n",
    "read_and_plot_chi_intensity(folder, patterns, normalize=True)    \n",
    "\n",
    "patterns = [#'*cryomill_P3HB_powder*_s00010*',\n",
    " '*f_P3HB_*_50RPM*_s00010*',\n",
    " '*f_P3HB_*_100RPM*_s00012*',\n",
    " '*f_P3HB_*_200RPM*_Scan00005_s00010*',\n",
    " '*f_P3HB_*_400RPM*_s00005*',\n",
    " '*f_P3HB_*_600RPM*_s00007*'\n",
    "]\n",
    "\n",
    "read_and_plot_chi_intensity(folder, patterns, normalize=True) \n",
    "\n",
    "patterns = [#'*cryomill_PET_powder*_s00010*',\n",
    " '*PET_*_50rpm*_s00013*',\n",
    " '*PET_*_100rpm*_s00004*',\n",
    " '*PET_*_200rpm*_s00001*',\n",
    " '*PET_*_400rpm*_s00007*',\n",
    " '*PET_*_600rpm*_s00005*'\n",
    "]\n",
    "\n",
    "read_and_plot_chi_intensity(folder, patterns, normalize=True)     \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bl17_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
