{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 reduction complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from param import output\n",
    "import pyFAI\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "import fabio\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Utility Functions ---\n",
    "\n",
    "def set_plot_style(axs, fonts, xlabel, ylabel):\n",
    "    axs.set_xlabel(xlabel, fontsize=fonts)\n",
    "    axs.set_ylabel(ylabel, fontsize=fonts)\n",
    "    axs.tick_params(axis='both', which='major', direction='out', length=4, width=1)\n",
    "    axs.tick_params(which='minor', width=1, size=2)\n",
    "    axs.minorticks_on()\n",
    "    axs.set_facecolor('white')\n",
    "    for key in axs.spines:\n",
    "        axs.spines[key].set_linewidth(1)\n",
    "    axs.tick_params(axis='x', labelsize=fonts)\n",
    "    axs.tick_params(axis='y', labelsize=fonts)\n",
    "    return axs\n",
    "\n",
    "def plot_2d_images(image, poni_file, output_folder, file_label, azm_range, vmin=None, vmax=None):\n",
    "    ai = pyFAI.load(poni_file)\n",
    "    pixel_size = ai.pixel1\n",
    "    detector_distance = ai.dist\n",
    "    beamx = ai.getFit2D()['centerX']\n",
    "    beamy = ai.getFit2D()['centerY']\n",
    "    wavelength = ai.wavelength\n",
    "    x_pixels = image.shape[1]\n",
    "    y_pixels = image.shape[0]\n",
    "    x_coords = np.arange(x_pixels) - beamx\n",
    "    y_coords = np.arange(y_pixels) - beamy\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "    qx = 1e-9 * 2 * np.pi / wavelength * np.sin(pixel_size * xx / detector_distance)\n",
    "    qy = 1e-9 * 2 * np.pi / wavelength * np.sin(pixel_size * yy / detector_distance)\n",
    "    azimuthal_angles = np.degrees(np.arctan2(qy, qx))\n",
    "    mask = (azimuthal_angles >= azm_range[0]) & (azimuthal_angles <= azm_range[1])\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    image = np.nan_to_num(image, nan=0.0)\n",
    "    image = 10000000 * (image / np.sum(image))\n",
    "    im_avg = ax.pcolormesh(qx, qy, image, cmap='jet', vmin=vmin, vmax=vmax, shading='auto')\n",
    "    ax.set_aspect('equal')\n",
    "    cbar_avg = fig.colorbar(im_avg, ax=ax, shrink=.8, label='Intensity [a.u.]')\n",
    "    cbar_avg.ax.tick_params(labelsize=20)\n",
    "    cbar_avg.ax.yaxis.label.set_size(20)\n",
    "    ax.set_xlabel(r\"$q_x$ [nm$^{-1}$]\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$q_y$ [nm$^{-1}$]\", fontsize=20)\n",
    "    ax.set_title(f\"{file_label}_2D\", fontsize=14, y=1.05)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "    output_folder = os.path.join(output_folder, \"Figures\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    plt.savefig(f\"{output_folder}/{file_label}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# For sorting files consistently\n",
    "def sort_files(filenames):\n",
    "    import re\n",
    "    def extract_numbers(filename):\n",
    "        match = re.search(r'_s(\\d+)_0*(\\d+)', filename)\n",
    "        if match:\n",
    "            snumber = int(match.group(1))\n",
    "            number = int(match.group(2))\n",
    "            return snumber, number\n",
    "        return float('inf'), float('inf')\n",
    "    return sorted(filenames, key=extract_numbers)\n",
    "\n",
    "# --- TIFF Data Reading ---\n",
    "def get_tiff_img_data(fname):\n",
    "    with tifffile.TiffFile(fname) as tif:\n",
    "        for idx, page in enumerate(tif.pages):\n",
    "            if idx == 0:\n",
    "                img_data = page.asarray().astype(float)\n",
    "            else:\n",
    "                img_data = np.concatenate((img_data, page.asarray().astype(float)))\n",
    "    img_data[img_data > 1e8] = np.nan\n",
    "    return img_data\n",
    "\n",
    "# --- HDF5 Data Reading (Eiger/Linkam) ---\n",
    "def get_single_image_h5(fname, n, mask=None, threshold=1e8):\n",
    "    try:\n",
    "        imgs = fabio.open(fname)\n",
    "    except Exception:\n",
    "        return None\n",
    "    img = imgs.get_frame(n).data.astype(float)\n",
    "    img[img > threshold] = np.nan\n",
    "    if mask is not None:\n",
    "        img[mask == 1] = np.nan\n",
    "    return img\n",
    "\n",
    "def get_nImages(fname):\n",
    "    try:\n",
    "        return fabio.open(fname).nframes\n",
    "    except Exception:\n",
    "        return 1\n",
    "\n",
    "# --- Core Unified Integration Function ---\n",
    "def integrate_files_unified(\n",
    "        base_path, poni_file, mask_file, folder, keyword,\n",
    "        output_base_path, file_type='tiff', nsave2d=3, azm_range=(-60, 60),\n",
    "        q1=None, q2=None, vmin=None, vmax=None, plot=False, method='splitpixel',\n",
    "        npt=4000, threshold=1e8, parallel=False, reprocess=False):\n",
    "    ai = pyFAI.load(poni_file)\n",
    "    mask = fabio.open(mask_file).data\n",
    "    output_folder = os.path.join(output_base_path, 'OneD_integrated_WAXS_01', folder)\n",
    "    os.makedirs(os.path.join(output_folder, 'Figures'), exist_ok=True)\n",
    "\n",
    "    if file_type == 'tiff':\n",
    "        data_folder = os.path.join(base_path, folder, keyword, 'Threshold 1')\n",
    "        tif_files = [f for f in os.listdir(data_folder) if f.endswith('.tif') and not f.startswith('._')]\n",
    "        tif_files = sort_files(tif_files)\n",
    "        azimuthal_data, radial_data = [], []\n",
    "        for i, tif_file in enumerate(tqdm(tif_files, desc='Processing TIFF'), start=1):\n",
    "            file_path = os.path.join(data_folder, tif_file)\n",
    "            img = get_tiff_img_data(file_path)\n",
    "            img[mask == 1] = 0\n",
    "            # Metadata logic can be adapted here if needed\n",
    "            normfactor = 1.0\n",
    "            if i % nsave2d == 0:\n",
    "                plot_2d_images(img, poni_file, output_folder, tif_file, azm_range, vmin, vmax)\n",
    "            result = ai.integrate1d(img, npt, error_model='poisson',\n",
    "                                    correctSolidAngle=True, azimuth_range=azm_range,\n",
    "                                    normalization_factor=normfactor,\n",
    "                                    polarization_factor=0.95, method=method, mask=mask)\n",
    "            q, I_azimuthal, error_azm = result.radial, result.intensity, result.sigma\n",
    "            azimuthal_data.append((q, I_azimuthal, error_azm, tif_file))\n",
    "            chi, I_radial = ai.integrate_radial(\n",
    "                img,\n",
    "                npt=720,\n",
    "                radial_range=(q1, q2),\n",
    "                azimuth_range=(-180, 0),\n",
    "                mask=mask,\n",
    "                normalization_factor=normfactor,\n",
    "                correctSolidAngle=True,\n",
    "                polarization_factor=0.95,\n",
    "                method=method,\n",
    "                unit='chi_deg',\n",
    "                radial_unit='q_nm^-1')\n",
    "            radial_data.append((chi, I_radial, tif_file))\n",
    "        print('TIFF reduction complete.')\n",
    "        # Add your HDF5 output saving logic here if desired (using azimuthal_data and radial_data)\n",
    "        return azimuthal_data, radial_data\n",
    "\n",
    "    elif file_type == 'h5':\n",
    "        h5_pattern = os.path.join(base_path, folder, f'*{keyword}*_master.h5')\n",
    "        h5_files = glob.glob(h5_pattern)\n",
    "\n",
    "        azimuthal_data, radial_data = [], []   # initialize here\n",
    "\n",
    "        for h5_file in h5_files:\n",
    "            nframes = get_nImages(h5_file)\n",
    "\n",
    "            def integrate_h5_frame(n):\n",
    "                img = get_single_image_h5(h5_file, n, mask, threshold=threshold)\n",
    "                if img is None:\n",
    "                    return None\n",
    "                result = ai.integrate1d(img, npt, correctSolidAngle=True, azimuth_range=azm_range,\n",
    "                                        polarization_factor=0.95, method=method, mask=mask, unit='q_nm^-1')\n",
    "                q, I = result.radial, result.intensity\n",
    "                chi, I_radial = ai.integrate_radial(\n",
    "                    img, npt=720, radial_range=(q1, q2), azimuth_range=(-180, 0),\n",
    "                    mask=mask, correctSolidAngle=True, polarization_factor=0.95,\n",
    "                    method=method, unit='chi_deg', radial_unit='q_nm^-1')\n",
    "                return (q, I, n), (chi, I_radial, n)\n",
    "\n",
    "            print(f'Processing {os.path.basename(h5_file)} ...')\n",
    "            if parallel:\n",
    "                results = Parallel(n_jobs=-1)(delayed(integrate_h5_frame)(n) for n in range(nframes))\n",
    "            else:\n",
    "                results = [integrate_h5_frame(n) for n in tqdm(range(nframes), desc='Processing HDF5')]\n",
    "\n",
    "            for res in results:\n",
    "                if res is None: \n",
    "                    continue\n",
    "                azm, rad = res\n",
    "                azimuthal_data.append(azm)\n",
    "                radial_data.append(rad)\n",
    "\n",
    "        print('HDF5 reduction complete.')\n",
    "        return azimuthal_data, radial_data\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"file_type must be 'tiff' or 'h5'.\")\n",
    "\n",
    "# --- Usage Example ---\n",
    "from pathlib import Path\n",
    "base_path = Path(r'/Volumes/SSD1/RawData1/Redesigned_Plastics/May2025/2025_05_Anjani')\n",
    "save_path  = base_path / 'processed_data'\n",
    "save_path.mkdir(exist_ok=True)\n",
    "\n",
    "calib_path = base_path / 'calibration'\n",
    "xye_path  = save_path / 'xye_data'\n",
    "xye_path.mkdir(exist_ok=True) \n",
    "\n",
    "calib_file = calib_path / 'LaB6_linkam_15kev.poni'\n",
    "dp_mask_file1 = calib_path / 'mask_01.edf'\n",
    "\n",
    "poni_file = str(calib_file)\n",
    "mask_file = str(dp_mask_file1)\n",
    "output_base_path = str(save_path)\n",
    "\n",
    "# For TIFF input:\n",
    "# integrate_files_unified(\n",
    "#     base_path, poni_file, mask_file, \"P3HB\", \"some_sample_keyword\",\n",
    "#     output_base_path, file_type='tiff', nsave2d=3, azm_range=(-60, 60),\n",
    "#     q1=9.25, q2=13, vmin=0, vmax=15, plot=True\n",
    "# \n",
    "\n",
    "# For HDF5 input:\n",
    "integrate_files_unified(\n",
    "     base_path, poni_file, mask_file, \"linkam\", \"Run8_LDPE_30C_50ums_scan001_master\",\n",
    "    output_base_path, file_type='h5', nsave2d=1, azm_range=(-45, 45),\n",
    "         q1=14, q2=16, vmin=0, vmax=2, plot=True, method='csr', npt=2000, threshold=1e8, parallel=True\n",
    " )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bl17_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
